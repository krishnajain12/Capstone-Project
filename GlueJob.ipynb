{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18277277",
   "metadata": {},
   "source": [
    "Task 1 - Employee_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e12b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_data/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_data_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# Expected columns\n",
    "expected_columns = {\"emp_id\", \"age\", \"name\"}\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee\"\n",
    "\n",
    "\n",
    "# Initialize Spark and boto3\n",
    "spark = SparkSession.builder.appName(\"DailyEmployeeProcessing\").getOrCreate()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Ensure processed files log exists\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = {line.split(',')[0]: float(line.split(',')[1]) for line in obj['Body'].read().decode('utf-8').splitlines() if line}\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        processed_files = {}\n",
    "\n",
    "    # List all CSV files in bronze\n",
    "    files = s3.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix).get('Contents', [])\n",
    "    csv_files = [f for f in files if f['Key'].endswith('.csv')]\n",
    "\n",
    "    # Identify new/updated files\n",
    "    to_process = []\n",
    "    for file in csv_files:\n",
    "        key = file['Key']\n",
    "        last_modified = file['LastModified'].timestamp()\n",
    "        if processed_files.get(key) != last_modified:\n",
    "            to_process.append((key, last_modified))\n",
    "\n",
    "    if not to_process:\n",
    "        print(\"INFO: No new or updated files to process. Exiting gracefully.\")\n",
    "    else:\n",
    "        for key, last_modified in to_process:\n",
    "            print(f\"Processing file: {key}\")\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"s3://{bucket_name}/{key}\")\n",
    "\n",
    "            # Check if all expected columns are present\n",
    "            actual_columns = set(df.columns)\n",
    "            missing = expected_columns - actual_columns\n",
    "            if missing:\n",
    "                raise ValueError(f\"ERROR: Missing expected columns in {key}: {missing}\")\n",
    "\n",
    "            # Clean and validate\n",
    "            cleaned_df = df.select(\n",
    "                col(\"emp_id\").cast(\"string\"),\n",
    "                col(\"age\").cast(\"int\"),\n",
    "                col(\"name\").cast(\"string\")\n",
    "            ).dropna().dropDuplicates()\n",
    "\n",
    "            # Write cleaned data to gold path\n",
    "            cleaned_df.write.mode(\"append\").parquet(gold_path)\n",
    "\n",
    "            #Write to Postgres\n",
    "            cleaned_df.write.mode(\"append\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "            # Update log\n",
    "            processed_files[key] = last_modified\n",
    "\n",
    "        # Write updated log back to S3\n",
    "        log_content = \"\\n\".join([f\"{k},{v}\" for k, v in processed_files.items()])\n",
    "        s3.put_object(Bucket=bucket_name, Key=processed_file_key, Body=log_content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Job failed with exception: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5335378",
   "metadata": {},
   "source": [
    "Task 2 - leave_quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88294028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import boto3\n",
    "\n",
    "# Define S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_leave_quota/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_leave_quota_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"leave_quota\"\n",
    "\n",
    "# Initialize Spark and Boto3\n",
    "spark = SparkSession.builder.appName(\"YearlyLeaveQuotaTableJob\").getOrCreate()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Step 1: Load processed files\n",
    "    try:\n",
    "        processed_obj = s3_client.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = processed_obj['Body'].read().decode('utf-8').splitlines()\n",
    "    except s3_client.exceptions.ClientError:\n",
    "        processed_files = []\n",
    "\n",
    "    processed_files_set = set(processed_files) \n",
    "\n",
    "    # Step 2: List all files under bronze_prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix)\n",
    "    all_files = response.get('Contents', [])\n",
    "\n",
    "    # Filter only CSV files excluding processed_files.txt itself\n",
    "    new_files = [obj['Key'] for obj in all_files if obj['Key'].endswith('.csv') and obj['Key'] not in processed_files_set]\n",
    "\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "    else:\n",
    "        for file_key in new_files:\n",
    "            file_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "            file_name = file_key.split(\"/\")[-1]  # Get only filename\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Step 3: Read the file\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_cols = [\"emp_id\", \"leave_quota\", \"year\"]\n",
    "            if not all(c in df.columns for c in required_cols):\n",
    "                print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Clean and transform\n",
    "            cleaned_df = (\n",
    "                df.select(\n",
    "                    col(\"emp_id\").cast(StringType()),\n",
    "                    col(\"leave_quota\").cast(IntegerType()),\n",
    "                    col(\"year\").cast(IntegerType())\n",
    "                )\n",
    "                .filter(\n",
    "                    col(\"emp_id\").isNotNull() &\n",
    "                    col(\"leave_quota\").isNotNull() &\n",
    "                    col(\"year\").isNotNull()\n",
    "                )\n",
    "                .dropDuplicates([\"emp_id\", \"year\"])\n",
    "            )\n",
    "\n",
    "            # Step 5: Write to Gold\n",
    "            print(f\"Writing {file_name} data to {gold_path}\")\n",
    "            cleaned_df.write.mode(\"append\").partitionBy(\"year\").parquet(gold_path)\n",
    "\n",
    "            #Write to Postgres\n",
    "            cleaned_df.write.mode(\"append\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "            # Step 6: After successful write, mark as processed\n",
    "            processed_files_set.add(file_key)\n",
    "\n",
    "        # Step 7: Update processed_files.txt\n",
    "        updated_content = \"\\n\".join(processed_files_set)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=processed_file_key, Body=updated_content)\n",
    "        print(\"Processed files list updated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4becf8",
   "metadata": {},
   "source": [
    "Task 3 - leave_calender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eedb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "import boto3\n",
    "\n",
    "# S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_leave_calendar_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"leave_calendar\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YearlyLeaveCalendarTableJob\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Step 1: Load processed files\n",
    "    try:\n",
    "        processed_obj = s3_client.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = processed_obj['Body'].read().decode('utf-8').splitlines()\n",
    "    except s3_client.exceptions.ClientError:\n",
    "        processed_files = []\n",
    "\n",
    "    processed_files_set = set(processed_files)\n",
    "\n",
    "    # Step 2: List all files under bronze_prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix)\n",
    "    all_files = response.get('Contents', [])\n",
    "\n",
    "    # Filter only CSV files excluding processed_files.txt itself\n",
    "    new_files = [obj['Key'] for obj in all_files if obj['Key'].endswith('.csv') and obj['Key'] not in processed_files_set]\n",
    "\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "    else:\n",
    "        for file_key in new_files:\n",
    "            file_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "            file_name = file_key.split(\"/\")[-1]  # Get only filename\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Step 3: Read the file\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_cols = [\"date\", \"reason\"]\n",
    "            if not all(c in df.columns for c in required_cols):\n",
    "                print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Clean and transform\n",
    "            cleaned_df = (\n",
    "                df.select(\n",
    "                    col(\"date\").cast(DateType()),\n",
    "                    col(\"reason\").cast(StringType())\n",
    "                )\n",
    "                .filter(\n",
    "                    col(\"date\").isNotNull() &\n",
    "                    col(\"reason\").isNotNull()\n",
    "                )\n",
    "                .dropDuplicates([\"date\", \"reason\"]) \n",
    "            )\n",
    "\n",
    "            # Add year column\n",
    "            cleaned_df = cleaned_df.withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "            # Step 5: Write to Gold\n",
    "            print(f\"Writing {file_name} data to {gold_path}\")\n",
    "            cleaned_df.write.mode(\"append\").partitionBy(\"year\").parquet(gold_path)\n",
    "\n",
    "            #Write to Postgres\n",
    "            cleaned_df.write.mode(\"append\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "            # Step 6: After successful write, mark as processed\n",
    "            processed_files_set.add(file_key)\n",
    "\n",
    "        # Step 7: Update processed_files.txt\n",
    "        updated_content = \"\\n\".join(processed_files_set)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=processed_file_key, Body=updated_content)\n",
    "        print(\"Processed files list updated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a21cd2",
   "metadata": {},
   "source": [
    "Task 4 - leave_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30982b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_date, to_date,\n",
    "    sum as sum_, when, lit, year, month, broadcast\n",
    ")\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "\n",
    "BRONZE_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_data/\"\n",
    "CONSOLIDATED = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee_leave_data\"\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder.appName(\"DailyLeaveSnapshot\").getOrCreate()\n",
    "\n",
    "# 1) LOAD raw events and cast date\n",
    "raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .csv(BRONZE_PATH)\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-M-d\"))\n",
    ")\n",
    "\n",
    "# 2) AGGREGATE to find dominant status per emp/date\n",
    "raw_agg = (\n",
    "    raw.groupBy(\"emp_id\", \"date\")\n",
    "    .agg(\n",
    "        sum_(when(col(\"status\") == \"ACTIVE\", 1).otherwise(0)).alias(\"cnt_active\"),\n",
    "        sum_(when(col(\"status\") == \"CANCELLED\", 1).otherwise(0)).alias(\"cnt_cancelled\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"new_status\",\n",
    "        when(col(\"cnt_active\") > col(\"cnt_cancelled\"), lit(\"ACTIVE\"))\n",
    "        .otherwise(lit(\"CANCELLED\"))  # includes tie and CANCELLED majority cases\n",
    "    )\n",
    "    .select(\"emp_id\", \"date\", \"new_status\")\n",
    ")\n",
    "\n",
    "# 3) READ previous snapshot (if any)\n",
    "try:\n",
    "    hist = (\n",
    "        spark.read\n",
    "        .parquet(CONSOLIDATED)\n",
    "        .select(\"emp_id\", \"date\", \"status\", \"ingestion_date\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    # If no history exists, create an empty DataFrame with the required schema.\n",
    "    hist = spark.createDataFrame(\n",
    "        [],\n",
    "        schema=raw_agg.schema.add(\"status\", StringType()).add(\"ingestion_date\", DateType())\n",
    "    )\n",
    "\n",
    "# 4) MERGE logic: outer-join of aggregated (raw_agg) & history\n",
    "merged = hist.alias(\"h\").join(\n",
    "    raw_agg.alias(\"r\"),\n",
    "    on=[\"emp_id\", \"date\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# 5) Decide final status and ingestion_date\n",
    "today = current_date()\n",
    "result = (\n",
    "    merged.withColumn(\n",
    "        \"final_status\",\n",
    "        when(col(\"r.new_status\").isNotNull(), col(\"r.new_status\"))\n",
    "        .otherwise(col(\"h.status\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ingestion_date\",\n",
    "        when(col(\"r.new_status\").isNotNull(), today)\n",
    "        .otherwise(col(\"h.ingestion_date\"))\n",
    "    )\n",
    "    .filter(col(\"final_status\").isNotNull())\n",
    "    .select(\n",
    "        col(\"emp_id\"),\n",
    "        col(\"date\"),\n",
    "        col(\"final_status\").alias(\"status\"),\n",
    "        col(\"ingestion_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 6) Add partitions based on the leave date\n",
    "result_with_partition = (\n",
    "    result\n",
    "    .withColumn(\"year\", year(col(\"date\")))\n",
    "    .withColumn(\"month\", month(col(\"date\")))\n",
    ")\n",
    "\n",
    "\n",
    "# Broadcast join with Employee Timeframe data\n",
    "employee_timeframe_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "\n",
    "# Load the employee timeframe data and filter to get only active employees.\n",
    "employee_active = (\n",
    "    spark.read.parquet(employee_timeframe_path)\n",
    "    .filter(col(\"status\") == \"ACTIVE\")\n",
    "    .select(\"emp_id\")\n",
    "    .distinct()\n",
    "    .withColumnRenamed(\"emp_id\", \"active_emp_id\")\n",
    ")\n",
    "\n",
    "# Perform a broadcast join to check if each employee in the leave data is active.\n",
    "# Then, update the leave status:\n",
    "# - If the employee is NOT active (no match found), force the status to \"CANCELLED\".\n",
    "# - Otherwise, retain the existing leave status from the aggregation logic.\n",
    "result_with_partition = result_with_partition.join(from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_date, to_date,\n",
    "    sum as sum_, when, lit, year, month, broadcast\n",
    ")\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "\n",
    "BRONZE_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_data/\"\n",
    "CONSOLIDATED = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee_leave_data\"\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder.appName(\"DailyLeaveSnapshot\").getOrCreate()\n",
    "\n",
    "# 1) LOAD raw events and cast date\n",
    "raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .csv(BRONZE_PATH)\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-M-d\"))\n",
    ")\n",
    "\n",
    "# 2) AGGREGATE to find dominant status per emp/date\n",
    "raw_agg = (\n",
    "    raw.groupBy(\"emp_id\", \"date\")\n",
    "    .agg(\n",
    "        sum_(when(col(\"status\") == \"ACTIVE\", 1).otherwise(0)).alias(\"cnt_active\"),\n",
    "        sum_(when(col(\"status\") == \"CANCELLED\", 1).otherwise(0)).alias(\"cnt_cancelled\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"new_status\",\n",
    "        when(col(\"cnt_active\") > col(\"cnt_cancelled\"), lit(\"ACTIVE\"))\n",
    "        .otherwise(lit(\"CANCELLED\"))  # includes tie and CANCELLED majority cases\n",
    "    )\n",
    "    .select(\"emp_id\", \"date\", \"new_status\")\n",
    ")\n",
    "\n",
    "# 3) READ previous snapshot (if any)\n",
    "try:\n",
    "    hist = (\n",
    "        spark.read\n",
    "        .parquet(CONSOLIDATED)\n",
    "        .select(\"emp_id\", \"date\", \"status\", \"ingestion_date\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    # If no history exists, create an empty DataFrame with the required schema.\n",
    "    hist = spark.createDataFrame(\n",
    "        [],\n",
    "        schema=raw_agg.schema.add(\"status\", StringType()).add(\"ingestion_date\", DateType())\n",
    "    )\n",
    "\n",
    "# 4) MERGE logic: outer-join of aggregated (raw_agg) & history\n",
    "merged = hist.alias(\"h\").join(\n",
    "    raw_agg.alias(\"r\"),\n",
    "    on=[\"emp_id\", \"date\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# 5) Decide final status and ingestion_date\n",
    "today = current_date()\n",
    "result = (\n",
    "    merged.withColumn(\n",
    "        \"final_status\",\n",
    "        when(col(\"r.new_status\").isNotNull(), col(\"r.new_status\"))\n",
    "        .otherwise(col(\"h.status\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ingestion_date\",\n",
    "        when(col(\"r.new_status\").isNotNull(), today)\n",
    "        .otherwise(col(\"h.ingestion_date\"))\n",
    "    )\n",
    "    .filter(col(\"final_status\").isNotNull())\n",
    "    .select(\n",
    "        col(\"emp_id\"),\n",
    "        col(\"date\"),\n",
    "        col(\"final_status\").alias(\"status\"),\n",
    "        col(\"ingestion_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 6) Add partitions based on the leave date\n",
    "result_with_partition = (\n",
    "    result\n",
    "    .withColumn(\"year\", year(col(\"date\")))\n",
    "    .withColumn(\"month\", month(col(\"date\")))\n",
    ")\n",
    "\n",
    "\n",
    "# Broadcast join with Employee Timeframe data\n",
    "employee_timeframe_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "\n",
    "# Load the employee timeframe data and filter to get only active employees.\n",
    "employee_active = (\n",
    "    spark.read.parquet(employee_timeframe_path)\n",
    "    .filter(col(\"status\") == \"ACTIVE\")\n",
    "    .select(\"emp_id\")\n",
    "    .distinct()\n",
    "    .withColumnRenamed(\"emp_id\", \"active_emp_id\")\n",
    ")\n",
    "\n",
    "# Perform a broadcast join to check if each employee in the leave data is active.\n",
    "# Then, update the leave status:\n",
    "# - If the employee is NOT active (no match found), force the status to \"CANCELLED\".\n",
    "# - Otherwise, retain the existing leave status from the aggregation logic.\n",
    "result_with_partition = result_with_partition.join(\n",
    "    broadcast(employee_active),\n",
    "    result_with_partition.emp_id == col(\"active_emp_id\"),\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"active_emp_id\").isNull(), lit(\"CANCELLED\")).otherwise(col(\"status\"))\n",
    ").drop(\"active_emp_id\")\n",
    "\n",
    "\n",
    "# 7) Write the final result to Gold with partitions\n",
    "\n",
    "\n",
    "(\n",
    "    result_with_partition\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .parquet(CONSOLIDATED)\n",
    ")\n",
    "\n",
    "#Write to Postgres\n",
    "result_with_partition.write.mode(\"overwrite\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "\n",
    "\n",
    ").withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"active_emp_id\").isNull(), lit(\"CANCELLED\")).otherwise(col(\"status\"))\n",
    ").drop(\"active_emp_id\")\n",
    "\n",
    "\n",
    "# 7) Write the final result to Gold with partitions\n",
    "\n",
    "\n",
    "(\n",
    "    result_with_partition\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .parquet(CONSOLIDATED)\n",
    ")\n",
    "\n",
    "#Write to Postgres\n",
    "result_with_partition.write.mode(\"overwrite\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1462e",
   "metadata": {},
   "source": [
    "Task 5 - timeframe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c33b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import psycopg2\n",
    "\n",
    "# Glue boilerplate\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Config\n",
    "config = {\n",
    "    \"pg_url\": \"jdbc:postgresql://54.174.233.120:5432/postgres\",\n",
    "    \"pg_properties\": {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"11223344\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    },\n",
    "    \"bucket_name\": \"poc-bootcamp-capstone-group3\",\n",
    "    \"bronze_prefix\": \"poc-bootcamp-bronze/employee_timeframe_data/\",\n",
    "    \"log_file_key\": \"poc-bootcamp-bronze/employee_timeframe_data/processed_files.txt\",\n",
    "    \"silver_prefix\": \"poc-bootcamp-silver/employee_timeframe_data/\"\n",
    "}\n",
    "\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Step 1: Get list of processed files\n",
    "def get_processed_files():\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=config['bucket_name'], Key=config['log_file_key'])\n",
    "        return {line.split(',')[0]: float(line.split(',')[1]) \n",
    "                for line in response['Body'].read().decode().splitlines() if line}\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        return {}\n",
    "\n",
    "# Step 2: List new/modified files in bronze\n",
    "def get_new_files():\n",
    "    processed_files = get_processed_files()\n",
    "    response = s3_client.list_objects_v2(Bucket=config['bucket_name'], Prefix=config['bronze_prefix'])\n",
    "    new_files = []\n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        if key.endswith(\".csv\") and key != config['log_file_key']:\n",
    "            modified_time = obj['LastModified'].timestamp()\n",
    "            if key not in processed_files or processed_files[key] != modified_time:\n",
    "                new_files.append(key)\n",
    "    return new_files\n",
    "\n",
    "# Step 3: Update log with newly processed files\n",
    "def update_processed_files(new_files):\n",
    "    processed_files = get_processed_files()\n",
    "    for file in new_files:\n",
    "        last_modified = s3_client.head_object(Bucket=config['bucket_name'], Key=file)['LastModified'].timestamp()\n",
    "        processed_files[file] = last_modified\n",
    "    log_content = \"\\n\".join(f\"{k},{v}\" for k, v in processed_files.items())\n",
    "    s3_client.put_object(Bucket=config['bucket_name'], Key=config['log_file_key'], Body=log_content)\n",
    "\n",
    "# Step 4: Read and process new CSV files\n",
    "new_files = get_new_files()\n",
    "if not new_files:\n",
    "    print(\"No new files to process.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "paths = [f\"s3://{config['bucket_name']}/{key}\" for key in new_files]\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(paths)\n",
    "\n",
    "# Step 5: Transformations\n",
    "df = df.withColumn(\"start_date\", to_date(from_unixtime(col(\"start_date\")))) \\\n",
    "        .withColumn(\"end_date\", to_date(from_unixtime(col(\"end_date\"))))\n",
    "\n",
    "window_spec = Window.partitionBy(\"emp_id\", \"start_date\", \"end_date\").orderBy(col(\"salary\").desc())\n",
    "df = df.withColumn(\"row_num\", row_number().over(window_spec)).filter(\"row_num = 1\").drop(\"row_num\")\n",
    "\n",
    "window_emp = Window.partitionBy(\"emp_id\").orderBy(\"start_date\")\n",
    "df = df.withColumn(\"next_start_date\", lead(\"start_date\").over(window_emp))\n",
    "df = df.withColumn(\"end_date\", when(col(\"end_date\").isNull(), col(\"next_start_date\")).otherwise(col(\"end_date\"))).drop(\"next_start_date\")\n",
    "\n",
    "df = df.withColumn(\"status\", when(col(\"end_date\").isNull(), \"ACTIVE\").otherwise(\"INACTIVE\")) \\\n",
    "       .select(\"emp_id\", \"start_date\", \"end_date\", \"designation\", \"salary\", \"status\")\n",
    "\n",
    "# Step 6: Write to Silver Layer in S3\n",
    "silver_path = f\"s3://{config['bucket_name']}/{config['silver_prefix']}{today_date}/\"\n",
    "df.write.mode(\"append\").parquet(silver_path)\n",
    "print(f\"Written processed data to Silver at {silver_path}\")\n",
    "\n",
    "\n",
    "db_config = {\n",
    "    \"host\": \"54.174.233.120\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\"\n",
    "}\n",
    "print(\"db_config done\")\n",
    "\n",
    "\n",
    "\n",
    "staging_table = \"employee_time_staging\"\n",
    "main_table = \"employee_timeframe\"\n",
    "\n",
    "\n",
    "\n",
    "min_id, max_id = df.selectExpr(\"min(emp_id) as min_id\", \"max(emp_id) as max_id\").first()\n",
    "\n",
    "\n",
    "df = df.persist()\n",
    "\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"TRUNCATE TABLE {staging_table};\")\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"Truncate error: {e}\")\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Step 1: Write to staging table\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{db_config['host']}:{db_config['port']}/{db_config['database']}\") \\\n",
    "    .option(\"dbtable\", staging_table) \\\n",
    "    .option(\"user\", db_config[\"user\"]) \\\n",
    "    .option(\"password\", db_config[\"password\"]) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"partitionColumn\", \"emp_id\")\\\n",
    "    .option(\"lowerBound\", min_id) \\\n",
    "    .option(\"upperBound\", max_id) \\\n",
    "    .option(\"numPartitions\", \"8\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Written data to staging table in PostgreSQL.\")\n",
    "\n",
    "# Step 2: Connect to PostgreSQL and upsert into main table\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Create tables if they don't exist\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {main_table} (\n",
    "        emp_id BIGINT,\n",
    "        start_date DATE,\n",
    "        end_date DATE,\n",
    "        designation TEXT,\n",
    "        salary FLOAT,\n",
    "        status TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {staging_table} (\n",
    "        emp_id BIGINT,\n",
    "        start_date DATE,\n",
    "        end_date DATE,\n",
    "        designation TEXT,\n",
    "        salary FLOAT,\n",
    "        status TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Upsert logic\n",
    "    cur.execute(f\"\"\"\n",
    "   -- Update existing ACTIVE records to set their end_date to the new record's start_date\n",
    "UPDATE {main_table} m\n",
    "SET end_date = s.start_date,\n",
    "    status = 'INACTIVE'\n",
    "FROM {staging_table} s\n",
    "WHERE m.emp_id = s.emp_id\n",
    "  AND m.status = 'ACTIVE'\n",
    "  AND s.start_date > m.start_date;  -- Ensure the new record has a later start date than the existing one\n",
    "\n",
    "\n",
    "    -- Step 2: Insert new records from staging\n",
    "INSERT INTO {main_table} (emp_id, start_date, end_date, designation, salary, status)\n",
    "SELECT s.emp_id, \n",
    "       s.start_date, \n",
    "       s.end_date,\n",
    "       s.designation, \n",
    "       s.salary, \n",
    "       s.status\n",
    "FROM {staging_table} s\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during DB operations: {e}\")\n",
    "    conn.rollback()\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "df.unpersist()\n",
    "\n",
    "print(\"Data merged into employee_time table with correct ACTIVE/INACTIVE status.\")\n",
    "\n",
    "# Writing to gold\n",
    "\n",
    "gold_df= spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{db_config['host']}:{db_config['port']}/{db_config['database']}\") \\\n",
    "    .option(\"dbtable\", main_table) \\\n",
    "    .option(\"user\", db_config[\"user\"]) \\\n",
    "    .option(\"password\", db_config[\"password\"]) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "s3_output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "\n",
    "gold_df.write.mode(\"overwrite\").partitionBy(\"status\").parquet(s3_output_path)\n",
    "\n",
    "print(f\"Data exported successfully to: {s3_output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 9: Update log\n",
    "update_processed_files(new_files)\n",
    "print(\"Processed file log updated.\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2281a",
   "metadata": {},
   "source": [
    "Task 6 - Count by designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e79941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, current_date\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee_designation\"\n",
    "\n",
    "# Initialize Spark and Glue contexts\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Define S3 paths\n",
    "gold_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/daily_active_employees_by_designation_output/\"\n",
    "\n",
    "# Read Gold Layer timeframe data\n",
    "df = spark.read.parquet(gold_path)\n",
    "\n",
    "# Filter records where today is between start_date and (end_date is null)\n",
    "today = datetime.utcnow().date()\n",
    "active_employees_df = df.filter(\n",
    "    (col(\"start_date\") <= today) &\n",
    "    (col(\"end_date\").isNull())\n",
    ")\n",
    "\n",
    "# Group by designation and count\n",
    "summary_df = active_employees_df.groupBy(\"designation\").count().withColumnRenamed(\"count\", \"active_count\")\n",
    "\n",
    "# Add snapshot date column for partitioning\n",
    "summary_df = summary_df.withColumn(\"snapshot_date\", current_date())\n",
    "\n",
    "# Write to S3 partitioned by snapshot_date\n",
    "summary_df.write.mode(\"append\").partitionBy(\"snapshot_date\").parquet(output_path)\n",
    "\n",
    "print(\"Daily active employee snapshot generated and saved to S3.\")\n",
    "\n",
    "#Write to Postgres\n",
    "summary_df.write.mode(\"overwrite\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "# Commit job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48653999",
   "metadata": {},
   "source": [
    "Task 7 - 8% Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, dayofweek, lit\n",
    "from pyspark.sql.types import DateType\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee_ex\"\n",
    "\n",
    "# Initialize Spark & Glue Context\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Get job parameters\n",
    "args = getResolvedOptions(sys.argv, ['YEAR', 'TODAY_DATE'])\n",
    "\n",
    "# PARAMETERS\n",
    "YEAR = int(args['YEAR'])  # Convert YEAR to integer\n",
    "THRESHOLD_PCT =0.08\n",
    "today = datetime.datetime.strptime(args['TODAY_DATE'], '%Y-%m-%d').date()  # Convert string to date\n",
    "\n",
    "# FILE PATHS\n",
    "leaves_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "holidays_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "http://54.174.233.120:3000/public-dashboards/c851c17993d4414db5a1eda9aabba085\n",
    "# LOAD LEAVE DATA \n",
    "leaves_df = spark.read.parquet(leaves_path).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# LOAD HOLIDAY DATA \n",
    "holidays_df = spark.read.option(\"header\", True).csv(holidays_path).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# BUILD CALENDAR (Excluding weekends and holidays)\n",
    "start = today + datetime.timedelta(days=1)  # Start from tomorrow\n",
    "end = datetime.date(YEAR, 12, 31)  # End on December 31\n",
    "\n",
    "# Generate range of dates from tomorrow to Dec 31\n",
    "days_df = spark.range(0, (end - start).days + 1).select((lit(start) + col(\"id\").cast(\"int\")).alias(\"date\"))\n",
    "\n",
    "# Filter out weekends and holidays\n",
    "working_days_df = days_df.join(holidays_df, on=\"date\", how=\"left_anti\").filter(dayofweek(col(\"date\")).between(2, 6))\n",
    "\n",
    "total_working_days = working_days_df.count()\n",
    "print(\"Total upcoming working days:\", total_working_days)\n",
    "\n",
    "# FILTER ACTIVE LEAVES ON FUTURE WORKING DAYS (Excluding cancelled)\n",
    "active_leaves_df = (\n",
    "    leaves_df.filter((col(\"status\") == \"ACTIVE\") & (col(\"date\") > lit(today)))\n",
    "    .join(working_days_df, on=\"date\", how=\"inner\")\n",
    "    .dropDuplicates([\"emp_id\", \"date\"])\n",
    ")\n",
    "\n",
    "# COUNT LEAVE APPLICATIONS PER EMPLOYEE\n",
    "emp_leave_counts_df = active_leaves_df.groupBy(\"emp_id\").agg(countDistinct(\"date\").alias(\"upcoming_leaves\"))\n",
    "\n",
    "# APPLY 8% THRESHOLD\n",
    "result_df = emp_leave_counts_df.filter(col(\"upcoming_leaves\") > THRESHOLD_PCT * lit(total_working_days))\n",
    "\n",
    "# Write output to S3 in Parquet format\n",
    "output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/8%-Threshold_output/\"\n",
    "result_df.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "#Write to Postgres\n",
    "result_df.write.mode(\"overwrite\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d9da2",
   "metadata": {},
   "source": [
    "Task 8 - 80% Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    to_date, col, year, month, countDistinct, concat_ws, lit, date_format\n",
    ")\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee_leaves_exceeding_80\"\n",
    "\n",
    "# Glue setup\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# S3 Config\n",
    "LEAVE_DATA_PATH     = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "LEAVE_CALENDAR_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "LEAVE_QUOTA_PATH    = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_quota/\"\n",
    "\n",
    "ALERT_TEXT_OUTPUT_PATH   = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_80%_text_output/\"\n",
    "PARQUET_OUTPUT_PATH      = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_80%_parquet_output/\"\n",
    "METADATA_KEY             = \"poc-bootcamp-bronze/80%Threshold/metadata.txt\"\n",
    "ALERTS_BUCKET            = \"poc-bootcamp-capstone-group3\"\n",
    "\n",
    "# Reference date and reporting period\n",
    "ref_date = datetime.date(2024, 11, 1)\n",
    "# ref_date = datetime.date.today()\n",
    "\n",
    "report_month = ref_date.month - 1 or 12\n",
    "report_year = ref_date.year if ref_date.month > 1 else ref_date.year - 1\n",
    "period = f\"{report_year}-{report_month:02d}\"\n",
    "\n",
    "# Load previously processed metadata\n",
    "s3 = boto3.client(\"s3\")\n",
    "processed = set()\n",
    "tmp_meta = \"/tmp/metadata.txt\"\n",
    "try:\n",
    "    s3.download_file(ALERTS_BUCKET, METADATA_KEY, tmp_meta)\n",
    "    with open(tmp_meta, \"r\") as f:\n",
    "        processed = set(line.strip() for line in f)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] != 'NoSuchKey':\n",
    "        raise\n",
    "\n",
    "# Load and clean data\n",
    "leave_df = (\n",
    "    spark.read.parquet(LEAVE_DATA_PATH)\n",
    "         .withColumn(\"date\", to_date(\"date\", \"yyyy-M-d\"))\n",
    "         .filter(col(\"status\") == \"ACTIVE\")\n",
    "         .dropDuplicates([\"emp_id\", \"date\"])\n",
    ")\n",
    "\n",
    "holidays_df = (\n",
    "    spark.read.option(\"header\", True).csv(LEAVE_CALENDAR_PATH)\n",
    "         .withColumn(\"date\", to_date(\"date\", \"yyyy-M-d\"))\n",
    "         .select(\"date\").distinct()\n",
    ")\n",
    "\n",
    "clean_leaves = (\n",
    "    leave_df.withColumn(\"dow\", date_format(\"date\", \"E\"))\n",
    "            .filter(~col(\"dow\").isin(\"Sat\", \"Sun\"))\n",
    "            .drop(\"dow\")\n",
    "            .join(holidays_df, on=\"date\", how=\"left_anti\")\n",
    ")\n",
    "\n",
    "quota_df = spark.read.option(\"header\", True).csv(LEAVE_QUOTA_PATH)\n",
    "\n",
    "# Filter only leaves in the current reporting year up to the reporting month\n",
    "up_to = clean_leaves.filter(\n",
    "    (year(\"date\") == report_year) &\n",
    "    (month(\"date\") <= report_month)\n",
    ")\n",
    "\n",
    "# Count leave days per employee\n",
    "counts_df = up_to.groupBy(\"emp_id\").agg(countDistinct(\"date\").alias(\"used\"))\n",
    "\n",
    "# Compare with quota and find violators\n",
    "breachers_df = (\n",
    "    counts_df.join(quota_df, on=\"emp_id\", how=\"inner\")\n",
    "             .filter((col(\"used\") / col(\"leave_quota\")) > 0.8)\n",
    "             .select(\"emp_id\")\n",
    ")\n",
    "\n",
    "# Avoid duplicates\n",
    "to_alert = [\n",
    "    emp for emp in breachers_df.collect()\n",
    "    if f\"{period},{emp.emp_id}\" not in processed\n",
    "]\n",
    "\n",
    "# Process alerts if any\n",
    "if to_alert:\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    text_out_path = ALERT_TEXT_OUTPUT_PATH \n",
    "\n",
    "    lines_df = spark.createDataFrame(\n",
    "        [(e.emp_id, period) for e in to_alert],\n",
    "        [\"emp_id\", \"month\"]\n",
    "    ).select(concat_ws(\",\", \"emp_id\", \"month\").alias(\"line\"))\n",
    "\n",
    "    lines_df.write.mode(\"append\").text(text_out_path)\n",
    "    print(f\"Alerted {len(to_alert)} employees → {text_out_path}\")\n",
    "\n",
    "    # Write Parquet of employees who breached the limit with their used leave count\n",
    "    breachers_with_count_df = (\n",
    "        breachers_df.join(counts_df, on=\"emp_id\", how=\"inner\")\n",
    "                    .select(\n",
    "                        col(\"emp_id\"),\n",
    "                        col(\"used\").alias(\"count_of_leaves\"),\n",
    "                        lit(report_year).alias(\"year\"),\n",
    "                        lit(report_month).alias(\"month\")\n",
    "                    )\n",
    "                    .filter(col(\"emp_id\").isin([e.emp_id for e in to_alert]))\n",
    "    )\n",
    "\n",
    "    parquet_out_path = PARQUET_OUTPUT_PATH \n",
    "    breachers_with_count_df.write.partitionBy(\"year\", \"month\").mode(\"append\").parquet(parquet_out_path)\n",
    "    print(f\"Parquet file written with leave counts → {parquet_out_path}\")\n",
    "\n",
    "    #Write to Postgres\n",
    "    breachers_with_count_df.write.mode(\"append\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "    # ── 11) Update metadata\n",
    "    with open(tmp_meta, \"a\") as f:\n",
    "        for emp in to_alert:\n",
    "            f.write(f\"{period},{emp.emp_id}\\n\")\n",
    "    s3.upload_file(tmp_meta, ALERTS_BUCKET, METADATA_KEY)\n",
    "    os.remove(tmp_meta)\n",
    "else:\n",
    "    print(f\"No new alerts for {period}\")\n",
    "\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b6dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955c2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a51f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726740e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
