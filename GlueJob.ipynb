{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18277277",
   "metadata": {},
   "source": [
    "Task 1 - Employee_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e12b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_data/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_data_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# Expected columns\n",
    "expected_columns = {\"emp_id\", \"age\", \"name\"}\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee\"\n",
    "\n",
    "\n",
    "# Initialize Spark and boto3\n",
    "spark = SparkSession.builder.appName(\"DailyEmployeeProcessing\").getOrCreate()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Ensure processed files log exists\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = {line.split(',')[0]: float(line.split(',')[1]) for line in obj['Body'].read().decode('utf-8').splitlines() if line}\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        processed_files = {}\n",
    "\n",
    "    # List all CSV files in bronze\n",
    "    files = s3.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix).get('Contents', [])\n",
    "    csv_files = [f for f in files if f['Key'].endswith('.csv')]\n",
    "\n",
    "    # Identify new/updated files\n",
    "    to_process = []\n",
    "    for file in csv_files:\n",
    "        key = file['Key']\n",
    "        last_modified = file['LastModified'].timestamp()\n",
    "        if processed_files.get(key) != last_modified:\n",
    "            to_process.append((key, last_modified))\n",
    "\n",
    "    if not to_process:\n",
    "        print(\"INFO: No new or updated files to process. Exiting gracefully.\")\n",
    "    else:\n",
    "        for key, last_modified in to_process:\n",
    "            print(f\"Processing file: {key}\")\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"s3://{bucket_name}/{key}\")\n",
    "\n",
    "            # Check if all expected columns are present\n",
    "            actual_columns = set(df.columns)\n",
    "            missing = expected_columns - actual_columns\n",
    "            if missing:\n",
    "                raise ValueError(f\"ERROR: Missing expected columns in {key}: {missing}\")\n",
    "\n",
    "            # Clean and validate\n",
    "            cleaned_df = df.select(\n",
    "                col(\"emp_id\").cast(\"string\"),\n",
    "                col(\"age\").cast(\"int\"),\n",
    "                col(\"name\").cast(\"string\")\n",
    "            ).dropna().dropDuplicates()\n",
    "\n",
    "            # Write cleaned data to gold path\n",
    "            cleaned_df.write.mode(\"append\").parquet(gold_path)\n",
    "\n",
    "            #Write to Postgres\n",
    "            cleaned_df.write.mode(\"append\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "            # Update log\n",
    "            processed_files[key] = last_modified\n",
    "\n",
    "        # Write updated log back to S3\n",
    "        log_content = \"\\n\".join([f\"{k},{v}\" for k, v in processed_files.items()])\n",
    "        s3.put_object(Bucket=bucket_name, Key=processed_file_key, Body=log_content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Job failed with exception: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5335378",
   "metadata": {},
   "source": [
    "Task 2 - leave_quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88294028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import boto3\n",
    "\n",
    "# Define S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_leave_quota/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_leave_quota_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"leave_quota\"\n",
    "\n",
    "# Initialize Spark and Boto3\n",
    "spark = SparkSession.builder.appName(\"YearlyLeaveQuotaTableJob\").getOrCreate()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Step 1: Load processed files\n",
    "    try:\n",
    "        processed_obj = s3_client.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = processed_obj['Body'].read().decode('utf-8').splitlines()\n",
    "    except s3_client.exceptions.ClientError:\n",
    "        processed_files = []\n",
    "\n",
    "    processed_files_set = set(processed_files) \n",
    "\n",
    "    # Step 2: List all files under bronze_prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix)\n",
    "    all_files = response.get('Contents', [])\n",
    "\n",
    "    # Filter only CSV files excluding processed_files.txt itself\n",
    "    new_files = [obj['Key'] for obj in all_files if obj['Key'].endswith('.csv') and obj['Key'] not in processed_files_set]\n",
    "\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "    else:\n",
    "        for file_key in new_files:\n",
    "            file_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "            file_name = file_key.split(\"/\")[-1]  # Get only filename\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Step 3: Read the file\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_cols = [\"emp_id\", \"leave_quota\", \"year\"]\n",
    "            if not all(c in df.columns for c in required_cols):\n",
    "                print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Clean and transform\n",
    "            cleaned_df = (\n",
    "                df.select(\n",
    "                    col(\"emp_id\").cast(StringType()),\n",
    "                    col(\"leave_quota\").cast(IntegerType()),\n",
    "                    col(\"year\").cast(IntegerType())\n",
    "                )\n",
    "                .filter(\n",
    "                    col(\"emp_id\").isNotNull() &\n",
    "                    col(\"leave_quota\").isNotNull() &\n",
    "                    col(\"year\").isNotNull()\n",
    "                )\n",
    "                .dropDuplicates([\"emp_id\", \"year\"])\n",
    "            )\n",
    "\n",
    "            # Step 5: Write to Gold\n",
    "            print(f\"Writing {file_name} data to {gold_path}\")\n",
    "            cleaned_df.write.mode(\"append\").partitionBy(\"year\").parquet(gold_path)\n",
    "\n",
    "            #Write to Postgres\n",
    "            cleaned_df.write.mode(\"append\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "            # Step 6: After successful write, mark as processed\n",
    "            processed_files_set.add(file_key)\n",
    "\n",
    "        # Step 7: Update processed_files.txt\n",
    "        updated_content = \"\\n\".join(processed_files_set)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=processed_file_key, Body=updated_content)\n",
    "        print(\"Processed files list updated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4becf8",
   "metadata": {},
   "source": [
    "Task 3 - leave_calender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eedb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "import boto3\n",
    "\n",
    "# S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_leave_calendar_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"leave_calendar\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YearlyLeaveCalendarTableJob\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Step 1: Load processed files\n",
    "    try:\n",
    "        processed_obj = s3_client.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = processed_obj['Body'].read().decode('utf-8').splitlines()\n",
    "    except s3_client.exceptions.ClientError:\n",
    "        processed_files = []\n",
    "\n",
    "    processed_files_set = set(processed_files)\n",
    "\n",
    "    # Step 2: List all files under bronze_prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix)\n",
    "    all_files = response.get('Contents', [])\n",
    "\n",
    "    # Filter only CSV files excluding processed_files.txt itself\n",
    "    new_files = [obj['Key'] for obj in all_files if obj['Key'].endswith('.csv') and obj['Key'] not in processed_files_set]\n",
    "\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "    else:\n",
    "        for file_key in new_files:\n",
    "            file_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "            file_name = file_key.split(\"/\")[-1]  # Get only filename\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Step 3: Read the file\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_cols = [\"date\", \"reason\"]\n",
    "            if not all(c in df.columns for c in required_cols):\n",
    "                print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Clean and transform\n",
    "            cleaned_df = (\n",
    "                df.select(\n",
    "                    col(\"date\").cast(DateType()),\n",
    "                    col(\"reason\").cast(StringType())\n",
    "                )\n",
    "                .filter(\n",
    "                    col(\"date\").isNotNull() &\n",
    "                    col(\"reason\").isNotNull()\n",
    "                )\n",
    "                .dropDuplicates([\"date\", \"reason\"]) \n",
    "            )\n",
    "\n",
    "            # Add year column\n",
    "            cleaned_df = cleaned_df.withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "            # Step 5: Write to Gold\n",
    "            print(f\"Writing {file_name} data to {gold_path}\")\n",
    "            cleaned_df.write.mode(\"append\").partitionBy(\"year\").parquet(gold_path)\n",
    "\n",
    "            #Write to Postgres\n",
    "            cleaned_df.write.mode(\"append\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "            # Step 6: After successful write, mark as processed\n",
    "            processed_files_set.add(file_key)\n",
    "\n",
    "        # Step 7: Update processed_files.txt\n",
    "        updated_content = \"\\n\".join(processed_files_set)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=processed_file_key, Body=updated_content)\n",
    "        print(\"Processed files list updated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a21cd2",
   "metadata": {},
   "source": [
    "Task 4 - leave_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30982b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_date, to_date, \n",
    "    sum as sum_, when, lit, year, month\n",
    ")\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# Config for S3 log tracking\n",
    "config = {\n",
    "    \"bucket_name\": \"poc-bootcamp-capstone-group3\",\n",
    "    \"bronze_prefix\": \"poc-bootcamp-bronze/employee_leave_data/\",\n",
    "    \"log_file_key\": \"logs/employee_leave_log.csv\"\n",
    "}\n",
    "\n",
    "# S3 Client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Functions for tracking processed files in the log\n",
    "def get_processed_files():\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=config['bucket_name'], Key=config['log_file_key'])\n",
    "        return {line.split(',')[0]: float(line.split(',')[1]) \n",
    "                for line in response['Body'].read().decode().splitlines() if line}\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        return {}\n",
    "\n",
    "def get_new_files():\n",
    "    processed_files = get_processed_files()\n",
    "    response = s3_client.list_objects_v2(Bucket=config['bucket_name'], Prefix=config['bronze_prefix'])\n",
    "    new_files = []\n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        if key.endswith(\".csv\") and key != config['log_file_key']:\n",
    "            modified_time = obj['LastModified'].timestamp()\n",
    "            if key not in processed_files or processed_files[key] != modified_time:\n",
    "                new_files.append(key)\n",
    "    return new_files\n",
    "\n",
    "def update_processed_files(new_files):\n",
    "    processed_files = get_processed_files()\n",
    "    for file in new_files:\n",
    "        last_modified = s3_client.head_object(Bucket=config['bucket_name'], Key=file)['LastModified'].timestamp()\n",
    "        processed_files[file] = last_modified\n",
    "    log_content = \"\\n\".join(f\"{k},{v}\" for k, v in processed_files.items())\n",
    "    s3_client.put_object(Bucket=config['bucket_name'], Key=config['log_file_key'], Body=log_content)\n",
    "\n",
    "# Paths\n",
    "BRONZE_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_data/\"\n",
    "SILVER_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-silver/employee_leave_data/\"\n",
    "CONSOLIDATED = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_host = \"54.174.233.120\"\n",
    "pg_port = \"5432\"\n",
    "pg_db = \"postgres\"\n",
    "pg_user = \"postgres\"\n",
    "pg_password = \"11223344\"\n",
    "pg_table = \"employee_leave_staging\"\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "new_files = get_new_files()\n",
    "if not new_files:\n",
    "    print(\"No new files to process.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "paths = [f\"s3://{config['bucket_name']}/{key}\" for key in new_files]    \n",
    "\n",
    "\n",
    "# 1) LOAD raw events and cast date\n",
    "raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .csv(paths)\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-M-d\"))\n",
    ")\n",
    "\n",
    "# 2) Load Active Employee IDs from Timeframe (Gold Layer)\n",
    "employee_timeframe_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "try:\n",
    "    employee_active = (\n",
    "        spark.read.parquet(employee_timeframe_path)\n",
    "        .filter(col(\"status\") == \"ACTIVE\")\n",
    "        .select(\"emp_id\")\n",
    "        .distinct()\n",
    "        .withColumnRenamed(\"emp_id\", \"active_emp_id\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    # If no active employees file, create an empty DataFrame\n",
    "    employee_schema = StructType([StructField(\"active_emp_id\", StringType(), True)])\n",
    "    employee_active = spark.createDataFrame([], employee_schema)\n",
    "\n",
    "# 3) use Broadcast join and AGGREGATE to find dominant status per emp_id/date\n",
    "raw_with_active = (\n",
    "    raw.join(broadcast(employee_active), raw.emp_id == employee_active.active_emp_id, \"left\")\n",
    "    .withColumn(\n",
    "        \"adjusted_status\",\n",
    "        when(col(\"active_emp_id\").isNull(), lit(\"CANCELLED\"))  # inactive → CANCELLED\n",
    "        .otherwise(col(\"status\"))  # active → keep original status\n",
    "    )\n",
    ")\n",
    "\n",
    "# Now perform aggregation on adjusted status\n",
    "raw_agg = (\n",
    "    raw_with_active\n",
    "    .groupBy(\"emp_id\", \"date\")\n",
    "    .agg(\n",
    "        sum_(when(col(\"adjusted_status\") == \"ACTIVE\", 1).otherwise(0)).alias(\"cnt_active\"),\n",
    "        sum_(when(col(\"adjusted_status\") == \"CANCELLED\", 1).otherwise(0)).alias(\"cnt_cancelled\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"new_status\",\n",
    "        when(col(\"cnt_active\") > col(\"cnt_cancelled\"), lit(\"ACTIVE\"))\n",
    "        .otherwise(lit(\"CANCELLED\"))\n",
    "    )\n",
    "    .select(\"emp_id\", \"date\", \"new_status\")\n",
    ")\n",
    "\n",
    "# 4) Write to Silver Layer (partitioned)\n",
    "raw_agg_with_partition = raw_agg.withColumn(\"ingestion_date\", current_date())\n",
    "raw_agg_with_partition.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(SILVER_PATH)\n",
    "\n",
    "# Use boto3 to list files in S3 bucket directory\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "prefix = \"poc-bootcamp-silver/employee_leave_data/\"\n",
    "\n",
    "# List files in the specified S3 path\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# Check if there are any files in the directory\n",
    "csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the specified S3 path!\")\n",
    "else:\n",
    "    print(f\"Found CSV files: {csv_files}\")\n",
    "    \n",
    "    file_key = csv_files[0]\n",
    "    \n",
    "    # Fetch the file using the full S3 key (Bucket + Key)\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    data = obj['Body'].read().decode('utf-8')\n",
    "\n",
    "# Use copy_from with StringIO buffer\n",
    "    buffer = StringIO(data)\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"11223344\",\n",
    "        host=\"54.174.233.120\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Step 1: Create table if not exists\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS employee_leave_staging (\n",
    "        emp_id BIGINT,\n",
    "        date DATE,\n",
    "        new_status VARCHAR(50),\n",
    "        ingestion_date DATE\n",
    "    );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    \n",
    "    # Step 2: Truncate the table\n",
    "    cur.execute(\"TRUNCATE TABLE employee_leave_staging;\")\n",
    "    conn.commit()\n",
    "    \n",
    "    # Step 3: Copy data from CSV (assuming tab-delimited, no header)\n",
    "    next(buffer)\n",
    "    cur.copy_from(buffer, 'employee_leave_staging', sep=',')\n",
    "    \n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"CSV data loaded into PostgreSQL successfully!\")\n",
    "\n",
    "\n",
    "pg_conn = psycopg2.connect(\n",
    "    dbname=pg_db,\n",
    "    user=pg_user,\n",
    "    password=pg_password,\n",
    "    host=pg_host,\n",
    "    port=pg_port\n",
    ")\n",
    "pg_cursor = pg_conn.cursor()\n",
    "\n",
    "# SQL for creating the table if it doesn't exist\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS employee_leave_data1 (\n",
    "    emp_id BIGINT,\n",
    "    date DATE,\n",
    "    new_status VARCHAR(50),\n",
    "    ingestion_date DATE,\n",
    "    PRIMARY KEY (emp_id, date)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the CREATE TABLE statement\n",
    "try:\n",
    "    pg_cursor.execute(create_table_sql)\n",
    "    pg_conn.commit()  # Commit table creation if it was executed\n",
    "    print(\"Table created or already exists!\")\n",
    "except Exception as e:\n",
    "    pg_conn.rollback()\n",
    "    print(f\"Error creating table: {e}\")\n",
    "\n",
    "# Define SQL for upsert (insert or update)\n",
    "merge_sql = \"\"\"\n",
    "\n",
    "    INSERT INTO employee_leave_data1 (emp_id, date, \"new_status\", ingestion_date)\n",
    "    SELECT emp_id, date, \"new_status\", CURRENT_DATE\n",
    "    FROM employee_leave_staging\n",
    "    ON CONFLICT (emp_id, date)\n",
    "    DO UPDATE SET\n",
    "      \"new_status\" = EXCLUDED.\"new_status\",\n",
    "      ingestion_date = EXCLUDED.ingestion_date;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the merge SQL\n",
    "try:\n",
    "    pg_cursor.execute(merge_sql)\n",
    "    pg_conn.commit()  # Commit changes after the upsert\n",
    "    print(\"Upsert completed successfully!\")\n",
    "    update_processed_files(new_files)\n",
    "    \n",
    "    # Step 4: After upserting, read from the 'employee_leave_data1' table in PostgreSQL\n",
    "    print(\"Reading from employee_leave_data1 table...\")\n",
    "    \n",
    "    # JDBC URL for reading from PostgreSQL\n",
    "    pg_jdbc_url = f\"jdbc:postgresql://{pg_host}:{pg_port}/{pg_db}\"\n",
    "    \n",
    "    # Define properties for JDBC connection\n",
    "    pg_properties = {\n",
    "        \"user\": pg_user,\n",
    "        \"password\": pg_password,\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    \n",
    "    # Load the data from the PostgreSQL table into Spark\n",
    "    employee_leave_data = spark.read.jdbc(url=pg_jdbc_url, table=\"employee_leave_data1\", properties=pg_properties)\n",
    "    \n",
    "    # Step 6: Write the processed data to the Gold layer (final output)\n",
    "    print(f\"Writing processed data to Gold layer at {CONSOLIDATED}...\")\n",
    "    \n",
    "    # Write the processed data to the Gold layer in Parquet format for efficiency\n",
    "    employee_leave_data.write.mode(\"overwrite\").partitionBy(\"new_status\").parquet(CONSOLIDATED)\n",
    "\n",
    "    \n",
    "    print(\" Data successfully written to the Gold layer!\")\n",
    "\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    pg_conn.rollback()  # Rollback in case of error\n",
    "    print(f\"Error during upsert: {e}\")\n",
    "    \n",
    "\n",
    "# Close the connection\n",
    "pg_cursor.close()\n",
    "pg_conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1462e",
   "metadata": {},
   "source": [
    "Task 5 - timeframe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c33b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import psycopg2\n",
    "\n",
    "# Glue boilerplate\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Config\n",
    "config = {\n",
    "    \"pg_url\": \"jdbc:postgresql://54.174.233.120:5432/postgres\",\n",
    "    \"pg_properties\": {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"11223344\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    },\n",
    "    \"bucket_name\": \"poc-bootcamp-capstone-group3\",\n",
    "    \"bronze_prefix\": \"poc-bootcamp-bronze/employee_timeframe_data/\",\n",
    "    \"log_file_key\": \"poc-bootcamp-bronze/employee_timeframe_data/processed_files.txt\",\n",
    "    \"silver_prefix\": \"poc-bootcamp-silver/employee_timeframe_data/\"\n",
    "}\n",
    "\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Step 1: Get list of processed files\n",
    "def get_processed_files():\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=config['bucket_name'], Key=config['log_file_key'])\n",
    "        return {line.split(',')[0]: float(line.split(',')[1]) \n",
    "                for line in response['Body'].read().decode().splitlines() if line}\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        return {}\n",
    "\n",
    "# Step 2: List new/modified files in bronze\n",
    "def get_new_files():\n",
    "    processed_files = get_processed_files()\n",
    "    response = s3_client.list_objects_v2(Bucket=config['bucket_name'], Prefix=config['bronze_prefix'])\n",
    "    new_files = []\n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        if key.endswith(\".csv\") and key != config['log_file_key']:\n",
    "            modified_time = obj['LastModified'].timestamp()\n",
    "            if key not in processed_files or processed_files[key] != modified_time:\n",
    "                new_files.append(key)\n",
    "    return new_files\n",
    "\n",
    "# Step 3: Update log with newly processed files\n",
    "def update_processed_files(new_files):\n",
    "    processed_files = get_processed_files()\n",
    "    for file in new_files:\n",
    "        last_modified = s3_client.head_object(Bucket=config['bucket_name'], Key=file)['LastModified'].timestamp()\n",
    "        processed_files[file] = last_modified\n",
    "    log_content = \"\\n\".join(f\"{k},{v}\" for k, v in processed_files.items())\n",
    "    s3_client.put_object(Bucket=config['bucket_name'], Key=config['log_file_key'], Body=log_content)\n",
    "\n",
    "# Step 4: Read and process new CSV files\n",
    "new_files = get_new_files()\n",
    "if not new_files:\n",
    "    print(\"No new files to process.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "paths = [f\"s3://{config['bucket_name']}/{key}\" for key in new_files]\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(paths)\n",
    "\n",
    "# Step 5: Transformations\n",
    "df = df.withColumn(\"start_date\", to_date(from_unixtime(col(\"start_date\")))) \\\n",
    "        .withColumn(\"end_date\", to_date(from_unixtime(col(\"end_date\"))))\n",
    "\n",
    "window_spec = Window.partitionBy(\"emp_id\", \"start_date\", \"end_date\").orderBy(col(\"salary\").desc())\n",
    "df = df.withColumn(\"row_num\", row_number().over(window_spec)).filter(\"row_num = 1\").drop(\"row_num\")\n",
    "\n",
    "window_emp = Window.partitionBy(\"emp_id\").orderBy(\n",
    "    asc(\"start_date\"),\n",
    "    asc_nulls_last(\"end_date\")  # This will keep NULL end_dates at the end\n",
    ")\n",
    "df = df.withColumn(\"next_start_date\", lead(\"start_date\").over(window_emp))\n",
    "df = df.withColumn(\"end_date\", when(col(\"end_date\").isNull(), col(\"next_start_date\")).otherwise(col(\"end_date\"))).drop(\"next_start_date\")\n",
    "\n",
    "df = df.withColumn(\"status\", when(col(\"end_date\").isNull(), \"ACTIVE\").otherwise(\"INACTIVE\")) \\\n",
    "       .select(\"emp_id\", \"start_date\", \"end_date\", \"designation\", \"salary\", \"status\")\n",
    "\n",
    "# Step 6: Write to Silver Layer in S3\n",
    "silver_path = f\"s3://{config['bucket_name']}/{config['silver_prefix']}{today_date}/\"\n",
    "df.write.mode(\"append\").parquet(silver_path)\n",
    "print(f\"Written processed data to Silver at {silver_path}\")\n",
    "\n",
    "\n",
    "db_config = {\n",
    "    \"host\": \"54.174.233.120\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\"\n",
    "}\n",
    "print(\"db_config done\")\n",
    "\n",
    "\n",
    "\n",
    "staging_table = \"employee_time_staging\"\n",
    "main_table = \"employee_timeframe\"\n",
    "\n",
    "\n",
    "\n",
    "min_id, max_id = df.selectExpr(\"min(emp_id) as min_id\", \"max(emp_id) as max_id\").first()\n",
    "\n",
    "\n",
    "df = df.persist()\n",
    "\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"TRUNCATE TABLE {staging_table};\")\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"Truncate error: {e}\")\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Step 1: Write to staging table\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{db_config['host']}:{db_config['port']}/{db_config['database']}\") \\\n",
    "    .option(\"dbtable\", staging_table) \\\n",
    "    .option(\"user\", db_config[\"user\"]) \\\n",
    "    .option(\"password\", db_config[\"password\"]) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"partitionColumn\", \"emp_id\")\\\n",
    "    .option(\"lowerBound\", min_id) \\\n",
    "    .option(\"upperBound\", max_id) \\\n",
    "    .option(\"numPartitions\", \"8\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Written data to staging table in PostgreSQL.\")\n",
    "\n",
    "# Step 2: Connect to PostgreSQL and upsert into main table\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Create tables if they don't exist\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {main_table} (\n",
    "        emp_id BIGINT,\n",
    "        start_date DATE,\n",
    "        end_date DATE,\n",
    "        designation TEXT,\n",
    "        salary FLOAT,\n",
    "        status TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {staging_table} (\n",
    "        emp_id BIGINT,\n",
    "        start_date DATE,\n",
    "        end_date DATE,\n",
    "        designation TEXT,\n",
    "        salary FLOAT,\n",
    "        status TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Upsert logic\n",
    "    cur.execute(f\"\"\"\n",
    "   -- Update existing ACTIVE records to set their end_date to the new record's start_date\n",
    "UPDATE {main_table} m\n",
    "SET end_date = s.start_date,\n",
    "    status = 'INACTIVE'\n",
    "FROM {staging_table} s\n",
    "WHERE m.emp_id = s.emp_id\n",
    "  AND m.status = 'ACTIVE'\n",
    "  AND s.start_date > m.start_date;  -- Ensure the new record has a later start date than the existing one\n",
    "\n",
    "\n",
    "    -- Step 2: Insert new records from staging\n",
    "INSERT INTO {main_table} (emp_id, start_date, end_date, designation, salary, status)\n",
    "SELECT s.emp_id, \n",
    "       s.start_date, \n",
    "       s.end_date,\n",
    "       s.designation, \n",
    "       s.salary, \n",
    "       s.status\n",
    "FROM {staging_table} s\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during DB operations: {e}\")\n",
    "    conn.rollback()\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "df.unpersist()\n",
    "\n",
    "print(\"Data merged into employee_time table with correct ACTIVE/INACTIVE status.\")\n",
    "\n",
    "# Writing to gold\n",
    "\n",
    "gold_df= spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{db_config['host']}:{db_config['port']}/{db_config['database']}\") \\\n",
    "    .option(\"dbtable\", main_table) \\\n",
    "    .option(\"user\", db_config[\"user\"]) \\\n",
    "    .option(\"password\", db_config[\"password\"]) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "s3_output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "\n",
    "gold_df.write.mode(\"overwrite\").partitionBy(\"status\").parquet(s3_output_path)\n",
    "\n",
    "print(f\"Data exported successfully to: {s3_output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 9: Update log\n",
    "update_processed_files(new_files)\n",
    "print(\"Processed file log updated.\")\n",
    "\n",
    "job.commit()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2281a",
   "metadata": {},
   "source": [
    "Task 6 - Count by designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e79941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, current_date\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee_designation\"\n",
    "\n",
    "# Initialize Spark and Glue contexts\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Define S3 paths\n",
    "gold_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/daily_active_employees_by_designation_output/\"\n",
    "\n",
    "# Read Gold Layer timeframe data\n",
    "df = spark.read.parquet(gold_path)\n",
    "\n",
    "# Filter records where today is between start_date and (end_date is null)\n",
    "today = datetime.utcnow().date()\n",
    "active_employees_df = df.filter(\n",
    "    (col(\"start_date\") <= today) &\n",
    "    (col(\"end_date\").isNull())\n",
    ")\n",
    "\n",
    "# Group by designation and count\n",
    "summary_df = active_employees_df.groupBy(\"designation\").count().withColumnRenamed(\"count\", \"active_count\")\n",
    "\n",
    "# Add snapshot date column for partitioning\n",
    "summary_df = summary_df.withColumn(\"snapshot_date\", current_date())\n",
    "\n",
    "# Write to S3 partitioned by snapshot_date\n",
    "summary_df.write.mode(\"append\").partitionBy(\"snapshot_date\").parquet(output_path)\n",
    "\n",
    "print(\"Daily active employee snapshot generated and saved to S3.\")\n",
    "\n",
    "#Write to Postgres\n",
    "summary_df.write.mode(\"overwrite\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "# Commit job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48653999",
   "metadata": {},
   "source": [
    "Task 7 - 8% Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, dayofweek, lit\n",
    "from pyspark.sql.types import DateType\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee_ex\"\n",
    "\n",
    "# Initialize Spark & Glue Context\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Get job parameters\n",
    "args = getResolvedOptions(sys.argv, ['YEAR', 'TODAY_DATE'])\n",
    "\n",
    "# PARAMETERS\n",
    "YEAR = int(args['YEAR'])  # Convert YEAR to integer\n",
    "THRESHOLD_PCT =0.08\n",
    "today = datetime.datetime.strptime(args['TODAY_DATE'], '%Y-%m-%d').date()  # Convert string to date\n",
    "\n",
    "# FILE PATHS\n",
    "leaves_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "holidays_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "\n",
    "# LOAD LEAVE DATA \n",
    "leaves_df = spark.read.parquet(leaves_path).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# LOAD HOLIDAY DATA \n",
    "holidays_df = spark.read.option(\"header\", True).csv(holidays_path).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# BUILD CALENDAR (Excluding weekends and holidays)\n",
    "start = today + datetime.timedelta(days=1)  # Start from tomorrow\n",
    "end = datetime.date(YEAR, 12, 31)  # End on December 31\n",
    "\n",
    "# Generate range of dates from tomorrow to Dec 31\n",
    "days_df = spark.range(0, (end - start).days + 1).select((lit(start) + col(\"id\").cast(\"int\")).alias(\"date\"))\n",
    "\n",
    "# Filter out weekends and holidays\n",
    "working_days_df = days_df.join(holidays_df, on=\"date\", how=\"left_anti\").filter(dayofweek(col(\"date\")).between(2, 6))\n",
    "\n",
    "total_working_days = working_days_df.count()\n",
    "print(\"Total upcoming working days:\", total_working_days)\n",
    "\n",
    "# FILTER ACTIVE LEAVES ON FUTURE WORKING DAYS (Excluding cancelled)\n",
    "active_leaves_df = (\n",
    "    leaves_df.filter((col(\"new_status\") == \"ACTIVE\") & (col(\"date\") > lit(today)))\n",
    "    .join(working_days_df, on=\"date\", how=\"inner\")\n",
    "    .dropDuplicates([\"emp_id\", \"date\"])\n",
    ")\n",
    "\n",
    "# COUNT LEAVE APPLICATIONS PER EMPLOYEE\n",
    "emp_leave_counts_df = active_leaves_df.groupBy(\"emp_id\").agg(countDistinct(\"date\").alias(\"upcoming_leaves\"))\n",
    "\n",
    "# APPLY 8% THRESHOLD\n",
    "result_df = emp_leave_counts_df.filter(col(\"upcoming_leaves\") > THRESHOLD_PCT * lit(total_working_days))\n",
    "result_df=result_df.dropDuplicates()\n",
    "\n",
    "# Write output to S3 in Parquet format\n",
    "output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/8%-Threshold_output/\"\n",
    "result_df.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "#Write to Postgres\n",
    "result_df.write.mode(\"overwrite\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d9da2",
   "metadata": {},
   "source": [
    "Task 8 - 80% Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    to_date, col, year, month, countDistinct, concat_ws, lit, date_format\n",
    ")\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_url = \"jdbc:postgresql://54.174.233.120:5432/postgres\"\n",
    "pg_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"11223344\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "pg_table = \"employee_leaves_exceeding_80\"\n",
    "\n",
    "# Glue setup\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# S3 Config\n",
    "LEAVE_DATA_PATH     = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "LEAVE_CALENDAR_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "LEAVE_QUOTA_PATH    = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_quota/\"\n",
    "\n",
    "ALERT_TEXT_OUTPUT_PATH   = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_80%_text_output/\"\n",
    "PARQUET_OUTPUT_PATH      = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_80%_parquet_output/\"\n",
    "METADATA_KEY             = \"poc-bootcamp-bronze/80%Threshold/metadata.txt\"\n",
    "ALERTS_BUCKET            = \"poc-bootcamp-capstone-group3\"\n",
    "\n",
    "# Reference date and reporting period\n",
    "#ref_date = datetime.date(2024, 11, 1)\n",
    "# ref_date = datetime.date.today()\n",
    "\n",
    "# Parse ref_date from job parameter\n",
    "try:\n",
    "    ref_date = datetime.datetime.strptime(args['ref_date'], \"%Y-%m-%d\").date()\n",
    "except ValueError:\n",
    "    raise ValueError(\"Invalid ref_date format. Use YYYY-MM-DD.\")\n",
    "\n",
    "\n",
    "report_month = ref_date.month - 1 or 12\n",
    "report_year = ref_date.year if ref_date.month > 1 else ref_date.year - 1\n",
    "period = f\"{report_year}-{report_month:02d}\"\n",
    "\n",
    "# Load previously processed metadata\n",
    "s3 = boto3.client(\"s3\")\n",
    "processed = set()\n",
    "tmp_meta = \"/tmp/metadata.txt\"\n",
    "try:\n",
    "    s3.download_file(ALERTS_BUCKET, METADATA_KEY, tmp_meta)\n",
    "    with open(tmp_meta, \"r\") as f:\n",
    "        processed = set(line.strip() for line in f)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] != 'NoSuchKey':\n",
    "        raise\n",
    "\n",
    "# Load and clean data\n",
    "leave_df = (\n",
    "    spark.read.parquet(LEAVE_DATA_PATH)\n",
    "         .withColumn(\"date\", to_date(\"date\", \"yyyy-M-d\"))\n",
    "         .filter(col(\"new_status\") == \"ACTIVE\")\n",
    "         .dropDuplicates([\"emp_id\", \"date\"])\n",
    ")\n",
    "\n",
    "holidays_df = (\n",
    "    spark.read.option(\"header\", True).csv(LEAVE_CALENDAR_PATH)\n",
    "         .withColumn(\"date\", to_date(\"date\", \"yyyy-M-d\"))\n",
    "         .select(\"date\").distinct()\n",
    ")\n",
    "\n",
    "clean_leaves = (\n",
    "    leave_df.withColumn(\"dow\", date_format(\"date\", \"E\"))\n",
    "            .filter(~col(\"dow\").isin(\"Sat\", \"Sun\"))\n",
    "            .drop(\"dow\")\n",
    "            .join(holidays_df, on=\"date\", how=\"left_anti\")\n",
    ")\n",
    "\n",
    "quota_df = spark.read.option(\"header\", True).csv(LEAVE_QUOTA_PATH)\n",
    "\n",
    "# Filter only leaves in the current reporting year up to the reporting month\n",
    "up_to = clean_leaves.filter(\n",
    "    (year(\"date\") == report_year) &\n",
    "    (month(\"date\") <= report_month)\n",
    ")\n",
    "\n",
    "# Count leave days per employee\n",
    "counts_df = up_to.groupBy(\"emp_id\").agg(countDistinct(\"date\").alias(\"used\"))\n",
    "\n",
    "# Compare with quota and find violators\n",
    "breachers_df = (\n",
    "    counts_df.join(quota_df, on=\"emp_id\", how=\"inner\")\n",
    "             .filter((col(\"used\") / col(\"leave_quota\")) > 0.8)\n",
    "             .select(\"emp_id\")\n",
    ")\n",
    "\n",
    "# Avoid duplicates\n",
    "to_alert = [\n",
    "    emp for emp in breachers_df.collect()\n",
    "    if f\"{period},{emp.emp_id}\" not in processed\n",
    "]\n",
    "\n",
    "# Process alerts if any\n",
    "if to_alert:\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    text_out_path = ALERT_TEXT_OUTPUT_PATH \n",
    "\n",
    "    lines_df = spark.createDataFrame(\n",
    "        [(e.emp_id, period) for e in to_alert],\n",
    "        [\"emp_id\", \"month\"]\n",
    "    ).select(concat_ws(\",\", \"emp_id\", \"month\").alias(\"line\"))\n",
    "\n",
    "    lines_df.write.mode(\"append\").text(text_out_path)\n",
    "    print(f\"Alerted {len(to_alert)} employees → {text_out_path}\")\n",
    "\n",
    "    # Write Parquet of employees who breached the limit with their used leave count\n",
    "    breachers_with_count_df = (\n",
    "        breachers_df.join(counts_df, on=\"emp_id\", how=\"inner\")\n",
    "                    .select(\n",
    "                        col(\"emp_id\"),\n",
    "                        col(\"used\").alias(\"count_of_leaves\"),\n",
    "                        lit(report_year).alias(\"year\"),\n",
    "                        lit(report_month).alias(\"month\")\n",
    "                    )\n",
    "                    .filter(col(\"emp_id\").isin([e.emp_id for e in to_alert]))\n",
    "    )\n",
    "    \n",
    "    breachers_with_count_df=breachers_with_count_df.dropDuplicates()\n",
    "\n",
    "    parquet_out_path = PARQUET_OUTPUT_PATH \n",
    "    breachers_with_count_df.write.partitionBy(\"year\", \"month\").mode(\"append\").parquet(parquet_out_path)\n",
    "    print(f\"Parquet file written with leave counts → {parquet_out_path}\")\n",
    "\n",
    "    #Write to Postgres\n",
    "    breachers_with_count_df.write.mode(\"append\").jdbc(url=pg_url, table=pg_table, properties=pg_properties)\n",
    "\n",
    "    # ── 11) Update metadata\n",
    "    with open(tmp_meta, \"a\") as f:\n",
    "        for emp in to_alert:\n",
    "            f.write(f\"{period},{emp.emp_id}\\n\")\n",
    "    s3.upload_file(tmp_meta, ALERTS_BUCKET, METADATA_KEY)\n",
    "    os.remove(tmp_meta)\n",
    "else:\n",
    "    print(f\"No new alerts for {period}\")\n",
    "\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b6dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955c2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a51f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726740e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
