{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18277277",
   "metadata": {},
   "source": [
    "Task 1 - Employee_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e12b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_data/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_data_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# Expected columns\n",
    "expected_columns = {\"emp_id\", \"age\", \"name\"}\n",
    "\n",
    "# Initialize Spark and boto3\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"DailyEmployeeProcessing\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryo.registrationRequired\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Ensure processed files log exists\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = {line.split(',')[0]: float(line.split(',')[1]) for line in obj['Body'].read().decode('utf-8').splitlines() if line}\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        processed_files = {}\n",
    "\n",
    "    # List all CSV files in bronze\n",
    "    files = s3.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix).get('Contents', [])\n",
    "    csv_files = [f for f in files if f['Key'].endswith('.csv')]\n",
    "\n",
    "    # Identify new/updated files\n",
    "    to_process = []\n",
    "    for file in csv_files:\n",
    "        key = file['Key']\n",
    "        last_modified = file['LastModified'].timestamp()\n",
    "        if processed_files.get(key) != last_modified:\n",
    "            to_process.append((key, last_modified))\n",
    "\n",
    "    if not to_process:\n",
    "        print(\"INFO: No new or updated files to process. Exiting gracefully.\")\n",
    "    else:\n",
    "        for key, last_modified in to_process:\n",
    "            print(f\"Processing file: {key}\")\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"s3://{bucket_name}/{key}\")\n",
    "\n",
    "            # Check if all expected columns are present\n",
    "            actual_columns = set(df.columns)\n",
    "            missing = expected_columns - actual_columns\n",
    "            if missing:\n",
    "                raise ValueError(f\"ERROR: Missing expected columns in {key}: {missing}\")\n",
    "\n",
    "            # Clean and validate\n",
    "            cleaned_df = df.select(\n",
    "                col(\"emp_id\").cast(\"string\"),import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_data/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_data_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# Expected columns\n",
    "expected_columns = {\"emp_id\", \"age\", \"name\"}\n",
    "\n",
    "# Initialize Spark and boto3\n",
    "spark = SparkSession.builder.appName(\"DailyEmployeeProcessing\").getOrCreate()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Ensure processed files log exists\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = {line.split(',')[0]: float(line.split(',')[1]) for line in obj['Body'].read().decode('utf-8').splitlines() if line}\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        processed_files = {}\n",
    "\n",
    "    # List all CSV files in bronze\n",
    "    files = s3.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix).get('Contents', [])\n",
    "    csv_files = [f for f in files if f['Key'].endswith('.csv')]\n",
    "\n",
    "    # Identify new/updated files\n",
    "    to_process = []\n",
    "    for file in csv_files:\n",
    "        key = file['Key']\n",
    "        last_modified = file['LastModified'].timestamp()\n",
    "        if processed_files.get(key) != last_modified:\n",
    "            to_process.append((key, last_modified))\n",
    "\n",
    "    if not to_process:\n",
    "        print(\"INFO: No new or updated files to process. Exiting gracefully.\")\n",
    "    else:\n",
    "        for key, last_modified in to_process:\n",
    "            print(f\"Processing file: {key}\")\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"s3://{bucket_name}/{key}\")\n",
    "\n",
    "            # Check if all expected columns are present\n",
    "            actual_columns = set(df.columns)\n",
    "            missing = expected_columns - actual_columns\n",
    "            if missing:\n",
    "                raise ValueError(f\"ERROR: Missing expected columns in {key}: {missing}\")\n",
    "\n",
    "            # Clean and validate\n",
    "            cleaned_df = df.select(\n",
    "                col(\"emp_id\").cast(\"string\"),\n",
    "                col(\"age\").cast(\"int\"),\n",
    "                col(\"name\").cast(\"string\")\n",
    "            ).dropna().dropDuplicates()\n",
    "\n",
    "            # Write cleaned data to gold path\n",
    "            cleaned_df.write.mode(\"append\").parquet(gold_path)\n",
    "\n",
    "            # Update log\n",
    "            processed_files[key] = last_modified\n",
    "\n",
    "        # Write updated log back to S3\n",
    "        log_content = \"\\n\".join([f\"{k},{v}\" for k, v in processed_files.items()])\n",
    "        s3.put_object(Bucket=bucket_name, Key=processed_file_key, Body=log_content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Job failed with exception: {e}\")\n",
    "\n",
    "\n",
    "                col(\"age\").cast(\"int\"),\n",
    "                col(\"name\").cast(\"string\")\n",
    "            ).dropna().dropDuplicates()\n",
    "\n",
    "            # Write cleaned data to gold path\n",
    "            cleaned_df.write.mode(\"append\").parquet(gold_path)\n",
    "\n",
    "            # Update log\n",
    "            processed_files[key] = last_modified\n",
    "\n",
    "        # Write updated log back to S3\n",
    "        log_content = \"\\n\".join([f\"{k},{v}\" for k, v in processed_files.items()])\n",
    "        s3.put_object(Bucket=bucket_name, Key=processed_file_key, Body=log_content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Job failed with exception: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5335378",
   "metadata": {},
   "source": [
    "Task 2 - leave_quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88294028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import boto3\n",
    "\n",
    "# Define S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_leave_quota/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_leave_quota_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# Initialize Boto3\n",
    "# Initialize Spark with Kryo serialization enabled\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"YearlyLeaveQuotaTableJob\")\n",
    "      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      .config(\"spark.kryo.registrationRequired\", \"true\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Step 1: Load processed files\n",
    "    try:\n",
    "        processed_obj = s3_client.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = processed_obj['Body'].read().decode('utf-8').splitlines()\n",
    "    except s3_client.exceptions.ClientError:\n",
    "        processed_files = []\n",
    "\n",
    "    processed_files_set = set(processed_files)  # Faster lookup\n",
    "\n",
    "    # Step 2: List all files under bronze_prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix)\n",
    "    all_files = response.get('Contents', [])\n",
    "\n",
    "    # Filter only CSV files excluding processed_files.txt itself\n",
    "    new_files = [obj['Key'] for obj in all_files if obj['Key'].endswith('.csv') and obj['Key'] not in processed_files_set]\n",
    "\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "    else:\n",
    "        for file_key in new_files:\n",
    "            file_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "            file_name = file_key.split(\"/\")[-1]  # Get only filename\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Step 3: Read the file\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_cols = [\"emp_id\", \"leave_quota\", \"year\"]\n",
    "            if not all(c in df.columns for c in required_cols):\n",
    "                print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Clean and transform\n",
    "            cleaned_df = (\n",
    "                df.select(\n",
    "                    col(\"emp_id\").cast(StringType()),\n",
    "                    col(\"leave_quota\").cast(IntegerType()),\n",
    "                    col(\"year\").cast(IntegerType())\n",
    "                )\n",
    "                .filter(\n",
    "                    col(\"emp_id\").isNotNull() &\n",
    "                    col(\"leave_quota\").isNotNull() &\n",
    "                    col(\"year\").isNotNull()\n",
    "                )\n",
    "                .dropDuplicates([\"emp_id\", \"year\"])\n",
    "            )\n",
    "\n",
    "            # Step 5: Write to Gold\n",
    "            print(f\"Writing {file_name} data to {gold_path}\")\n",
    "            cleaned_df.write.mode(\"append\").partitionBy(\"year\").parquet(gold_path)\n",
    "\n",
    "            # Step 6: After successful write, mark as processed\n",
    "            processed_files_set.add(file_key)\n",
    "\n",
    "        # Step 7: Update processed_files.txt\n",
    "        updated_content = \"\\n\".join(processed_files_set)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=processed_file_key, Body=updated_content)\n",
    "        print(\"Processed files list updated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4becf8",
   "metadata": {},
   "source": [
    "Task 3 - leave_calender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eedb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "import boto3\n",
    "\n",
    "# S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_leave_calendar_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"YearlyLeaveCalendarTableJob\")\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .config(\"spark.kryo.registrationRequired\", \"true\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Step 1: Load processed files\n",
    "    try:\n",
    "        processed_obj = s3_client.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = processed_obj['Body'].read().decode('utf-8').splitlines()\n",
    "    except s3_client.exceptions.ClientError:\n",
    "        processed_files = []\n",
    "\n",
    "    processed_files_set = set(processed_files)\n",
    "\n",
    "    # Step 2: List all files under bronze_prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix)\n",
    "    all_files = response.get('Contents', [])\n",
    "\n",
    "    # Filter only CSV files excluding processed_files.txt itself\n",
    "    new_files = [obj['Key'] for obj in all_files if obj['Key'].endswith('.csv') and obj['Key'] not in processed_files_set]\n",
    "\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "    else:\n",
    "        for file_key in new_files:\n",
    "            file_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "            file_name = file_key.split(\"/\")[-1]  # Get only filename\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Step 3: Read the file\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_cols = [\"date\", \"reason\"]\n",
    "            if not all(c in df.columns for c in required_cols):\n",
    "                print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Clean and transform\n",
    "            cleaned_df = (\n",
    "                df.select(\n",
    "                    col(\"date\").cast(DateType()),\n",
    "                    col(\"reason\").cast(StringType())\n",
    "                )\n",
    "                .filter(\n",
    "                    col(\"date\").isNotNull() &\n",
    "                    col(\"reason\").isNotNull()\n",
    "                )\n",
    "                .dropDuplicates([\"date\", \"reason\"]) \n",
    "            )\n",
    "\n",
    "            # Add year column\n",
    "            cleaned_df = cleaned_df.withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "            # Step 5: Write to Gold\n",
    "            print(f\"Writing {file_name} data to {gold_path}\")\n",
    "            cleaned_df.write.mode(\"append\").partitionBy(\"year\").parquet(gold_path)\n",
    "\n",
    "            # Step 6: After successful write, mark as processed\n",
    "            processed_files_set.add(file_key)\n",
    "\n",
    "        # Step 7: Update processed_files.txt\n",
    "        updated_content = \"\\n\".join(processed_files_set)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=processed_file_key, Body=updated_content)\n",
    "        print(\"Processed files list updated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a21cd2",
   "metadata": {},
   "source": [
    "Task 4 - leave_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30982b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_date, to_date,\n",
    "    sum as sum_, when, lit, year, month\n",
    ")\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Constants\n",
    "BRONZE_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_data/\"\n",
    "CONSOLIDATED = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "\n",
    "\n",
    "# Initialize Spark with Kryo serialization\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"DailyLeaveSnapshot\")\n",
    "      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "      .config(\"spark.kryo.registrationRequired\", \"true\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "# 1) LOAD raw events and cast date\n",
    "raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .csv(BRONZE_PATH)\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-M-d\"))\n",
    ")\n",
    "\n",
    "# 2) AGGREGATE to find dominant status per emp/date\n",
    "raw_agg = (\n",
    "    raw.groupBy(\"emp_id\", \"date\")\n",
    "    .agg(\n",
    "        sum_(when(col(\"status\") == \"ACTIVE\", 1).otherwise(0)).alias(\"cnt_active\"),\n",
    "        sum_(when(col(\"status\") == \"CANCELLED\", 1).otherwise(0)).alias(\"cnt_cancelled\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"new_status\",\n",
    "        when(col(\"cnt_active\") > col(\"cnt_cancelled\"), lit(\"ACTIVE\"))\n",
    "        .otherwise(lit(\"CANCELLED\"))  # Includes tie and CANCELLED majority cases\n",
    "    )\n",
    "    .select(\"emp_id\", \"date\", \"new_status\")\n",
    ")\n",
    "\n",
    "# 3) READ previous snapshot (if any)\n",
    "try:\n",
    "    hist = (\n",
    "        spark.read\n",
    "        .parquet(CONSOLIDATED)\n",
    "        .select(\"emp_id\", \"date\", \"status\", \"ingestion_date\")\n",
    "    )\n",
    "except:\n",
    "    hist = spark.createDataFrame(\n",
    "        [],\n",
    "        schema=raw_agg.schema.add(\"status\", StringType()).add(\"ingestion_date\", DateType())\n",
    "    )\n",
    "\n",
    "# 4) MERGE logic: outer-join agg & history\n",
    "merged = hist.alias(\"h\").join(\n",
    "    raw_agg.alias(\"r\"),\n",
    "    on=[\"emp_id\", \"date\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# 5) Decide final status and ingestion_date\n",
    "today = current_date()\n",
    "result = (\n",
    "    merged\n",
    "    .withColumn(\n",
    "        \"final_status\",\n",
    "        when(col(\"r.new_status\").isNotNull(), col(\"r.new_status\"))\n",
    "        .otherwise(col(\"h.status\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ingestion_date\",\n",
    "        when(col(\"r.new_status\").isNotNull(), today)\n",
    "        .otherwise(col(\"h.ingestion_date\"))\n",
    "    )\n",
    "    .filter(col(\"final_status\").isNotNull())\n",
    "    .select(\n",
    "        col(\"emp_id\"),\n",
    "        col(\"date\"),\n",
    "        col(\"final_status\").alias(\"status\"),\n",
    "        col(\"ingestion_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 6) Add partitions and write to Gold\n",
    "result_with_partition = (\n",
    "    result\n",
    "    .withColumn(\"year\", year(col(\"date\")))\n",
    "    .withColumn(\"month\", month(col(\"date\")))\n",
    ")\n",
    "\n",
    "(\n",
    "    result_with_partition\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .parquet(CONSOLIDATED)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1462e",
   "metadata": {},
   "source": [
    "Task 5 - timeframe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c33b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    to_date, col, year, month, countDistinct, concat_ws, lit, date_format, from_unixtime, expr, row_number, lead\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "# ── 1) Glue setup\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# ── Enable Kryo serialization\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "spark.conf.set(\"spark.kryo.registrationRequired\", \"true\")\n",
    "\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# ── 2) S3 Config\n",
    "bucket_name      = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_folder    = \"poc-bootcamp-bronze/employee_timeframe_data/\"\n",
    "silver_base      = f\"s3://{bucket_name}/poc-bootcamp-silver/employee_timeframe_data/\"\n",
    "gold_path        = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "log_file_key     = f\"{bronze_folder}processed_files.txt\"\n",
    "\n",
    "# ── 3) Today’s date for Silver path\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# ── 4) Ensure processed-files log exists\n",
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    s3.head_object(Bucket=bucket_name, Key=log_file_key)\n",
    "except s3.exceptions.ClientError:\n",
    "    s3.put_object(Bucket=bucket_name, Key=log_file_key, Body=\"\")\n",
    "\n",
    "# ── 5) Load processed-files log\n",
    "processed_obj       = s3.get_object(Bucket=bucket_name, Key=log_file_key)\n",
    "processed_lines     = processed_obj['Body'].read().decode().splitlines()\n",
    "processed_files     = {line.split(',')[0]: float(line.split(',')[1]) for line in processed_lines if line}\n",
    "  \n",
    "# ── 6) List new/modified CSVs in Bronze\n",
    "resp    = s3.list_objects_v2(Bucket=bucket_name, Prefix=bronze_folder)\n",
    "all_keys = [c['Key'] for c in resp.get('Contents', []) if c['Key'].endswith('.csv')]\n",
    "new_keys = []\n",
    "for key in all_keys:\n",
    "    lm = s3.head_object(Bucket=bucket_name, Key=key)['LastModified'].timestamp()\n",
    "    if processed_files.get(key) != lm:\n",
    "        new_keys.append((key, lm))\n",
    "\n",
    "if not new_keys:\n",
    "    print(\"🚫 No new files to process.\")\n",
    "else:\n",
    "    print(f\"✅ Found {len(new_keys)} new files.\")\n",
    "\n",
    "    # ── 7) Read new files into Silver DataFrame\n",
    "    paths     = [f\"s3://{bucket_name}/{key}\" for key, _ in new_keys]\n",
    "    silver_df = spark.read.csv(paths, header=True, inferSchema=True) \\\n",
    "        .withColumn(\"start_date\", to_date(from_unixtime(col(\"start_date\")))) \\\n",
    "        .withColumn(\"end_date\",   to_date(from_unixtime(col(\"end_date\"))))\n",
    "\n",
    "    # ── 8) Write Silver for today\n",
    "    silver_path = silver_base + today_date + \"/\"\n",
    "    silver_df.write.mode(\"append\").parquet(silver_path)\n",
    "    print(f\"✅ Silver written to {silver_path}\")\n",
    "\n",
    "    # ── 9) Update processed-files log\n",
    "    for key, lm in new_keys:\n",
    "        processed_files[key] = lm\n",
    "    log_body = \"\\n\".join(f\"{k},{v}\" for k, v in processed_files.items())\n",
    "    s3.put_object(Bucket=bucket_name, Key=log_file_key, Body=log_body)\n",
    "    print(\"📝 Updated processed-files log.\")\n",
    "\n",
    "    # ── 10) Silver ➔ Gold merge & dedupe\n",
    "    try:\n",
    "        existing_gold = spark.read.parquet(gold_path)\n",
    "    except:\n",
    "        existing_gold = spark.createDataFrame([], silver_df.schema.add(\"status\", \"string\"))\n",
    "\n",
    "    combined = existing_gold.unionByName(silver_df.withColumn(\"status\", lit(None)), allowMissingColumns=True)\n",
    "\n",
    "    # Deduplicate by highest salary per emp_id + timeframe\n",
    "    w1 = Window.partitionBy(\"emp_id\", \"start_date\", \"end_date\").orderBy(col(\"salary\").desc())\n",
    "    dedup = combined.withColumn(\"rn\", row_number().over(w1)).filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "    # Fill missing end_date\n",
    "    w2 = Window.partitionBy(\"emp_id\").orderBy(\"start_date\")\n",
    "    dedup = dedup.withColumn(\"next_start\", lead(\"start_date\").over(w2)) \\\n",
    "        .withColumn(\"end_date\", when(col(\"end_date\").isNull() & col(\"next_start\").isNotNull(),\n",
    "                                     expr(\"date_sub(next_start,1)\")).otherwise(col(\"end_date\")))\n",
    "\n",
    "    # Assign ACTIVE/INACTIVE\n",
    "    final_df = dedup.withColumn(\"status\",\n",
    "        when(col(\"end_date\").isNull(), \"ACTIVE\").otherwise(\"INACTIVE\")\n",
    "    ).select(\"emp_id\",\"start_date\",\"end_date\",\"designation\",\"salary\",\"status\")\n",
    "\n",
    "    # ── 11) Write Gold partitioned by status\n",
    "    final_df.write.mode(\"overwrite\").partitionBy(\"status\").parquet(gold_path)\n",
    "    print(\"🏆 Gold layer updated successfully.\")\n",
    "\n",
    "# ── 12) Commit Glue job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2281a",
   "metadata": {},
   "source": [
    "Task 6 - Count by designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e79941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, current_date\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "# Initialize Spark and Glue contexts\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# ✅ Enable Kryo Serialization and Snappy compression\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "sc._conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "\n",
    "# Initialize Glue Job\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Define S3 paths\n",
    "gold_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/daily_active_employees_by_designation_output/\"\n",
    "\n",
    "# Read Gold Layer timeframe data\n",
    "df = spark.read.parquet(gold_path)\n",
    "\n",
    "# Filter records where today is between start_date and (end_date is null)\n",
    "today = datetime.utcnow().date()\n",
    "active_employees_df = df.filter(\n",
    "    (col(\"start_date\") <= today) &\n",
    "    (col(\"end_date\").isNull())\n",
    ")\n",
    "\n",
    "# Group by designation and count\n",
    "summary_df = active_employees_df.groupBy(\"designation\").count().withColumnRenamed(\"count\", \"active_count\")\n",
    "\n",
    "# Add snapshot date column for partitioning\n",
    "summary_df = summary_df.withColumn(\"snapshot_date\", current_date())\n",
    "\n",
    "# Write to S3 partitioned by snapshot_date with Snappy compression\n",
    "summary_df.write.mode(\"append\").partitionBy(\"snapshot_date\").parquet(output_path)\n",
    "\n",
    "print(\"✅ Daily active employee snapshot generated and saved to S3.\")\n",
    "\n",
    "# Commit job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48653999",
   "metadata": {},
   "source": [
    "Task 7 - 8% Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, dayofweek, lit\n",
    "from pyspark.sql.types import DateType\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# Initialize Spark & Glue Context\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Enable Kryo serialization for optimized performance\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "spark.conf.set(\"spark.kryo.registrationRequired\", \"true\")\n",
    "\n",
    "# Get job parameters\n",
    "args = getResolvedOptions(sys.argv, ['YEAR', 'TODAY_DATE'])\n",
    "\n",
    "# PARAMETERS\n",
    "YEAR = int(args['YEAR'])  # Convert YEAR to integer\n",
    "THRESHOLD_PCT = 0.08\n",
    "today = datetime.datetime.strptime(args['TODAY_DATE'], '%Y-%m-%d').date()  # Convert string to date\n",
    "\n",
    "# FILE PATHS\n",
    "leaves_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "holidays_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "\n",
    "# LOAD LEAVE DATA (Parquet format)\n",
    "leaves_df = spark.read.parquet(leaves_path).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# LOAD HOLIDAY DATA (CSV format)\n",
    "holidays_df = spark.read.option(\"header\", True).csv(holidays_path).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# BUILD CALENDAR (Excluding weekends and holidays)\n",
    "start = today + datetime.timedelta(days=1)  # Start from tomorrow\n",
    "end = datetime.date(YEAR, 12, 31)  # End on December 31\n",
    "\n",
    "# Generate range of dates from tomorrow to Dec 31\n",
    "days_df = spark.range(0, (end - start).days + 1).select((lit(start) + col(\"id\").cast(\"int\")).alias(\"date\"))\n",
    "\n",
    "# Filter out weekends and holidays\n",
    "working_days_df = days_df.join(holidays_df, on=\"date\", how=\"left_anti\").filter(dayofweek(col(\"date\")).between(2, 6))\n",
    "\n",
    "total_working_days = working_days_df.count()\n",
    "print(\"Total upcoming working days:\", total_working_days)\n",
    "\n",
    "# FILTER ACTIVE LEAVES ON FUTURE WORKING DAYS (Excluding cancelled)\n",
    "active_leaves_df = (\n",
    "    leaves_df.filter((col(\"status\") == \"ACTIVE\") & (col(\"date\") > lit(today)))\n",
    "    .join(working_days_df, on=\"date\", how=\"inner\")\n",
    "    .dropDuplicates([\"emp_id\", \"date\"])\n",
    ")\n",
    "\n",
    "# COUNT LEAVE APPLICATIONS PER EMPLOYEE\n",
    "emp_leave_counts_df = active_leaves_df.groupBy(\"emp_id\").agg(countDistinct(\"date\").alias(\"upcoming_leaves\"))\n",
    "\n",
    "# APPLY 8% THRESHOLD\n",
    "result_df = emp_leave_counts_df.filter(col(\"upcoming_leaves\") > THRESHOLD_PCT * lit(total_working_days))\n",
    "\n",
    "# Write output to S3 in Parquet format\n",
    "output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/8%-Threshold_output/\"\n",
    "result_df.write.parquet(output_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d9da2",
   "metadata": {},
   "source": [
    "Task 8 - 80% Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    to_date, col, year, month, countDistinct, concat_ws, lit, date_format\n",
    ")\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# ── 1) Glue setup\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Enable Kryo serialization for optimized performance\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "spark.conf.set(\"spark.kryo.registrationRequired\", \"true\")\n",
    "\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# ── 2) S3 Config\n",
    "LEAVE_DATA_PATH     = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "LEAVE_CALENDAR_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "LEAVE_QUOTA_PATH    = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_quota/\"\n",
    "OUTPUT_PATH         = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_80%_output/\"\n",
    "METADATA_KEY        = \"poc-bootcamp-bronze/80%Threshold/metadata.txt\"\n",
    "ALERTS_BUCKET       = \"poc-bootcamp-capstone-group3\"\n",
    "\n",
    "# ── 3) Reference date and reporting period\n",
    "ref_date = datetime.date(2024,11, 1)  # <-- Replace with `datetime.date.today()` for real-time jobs\n",
    "# ref_date = datetime.date.today()   # <-- Uncomment to use today's date\n",
    "\n",
    "report_month = ref_date.month - 1 or 12\n",
    "report_year = ref_date.year if ref_date.month > 1 else ref_date.year - 1\n",
    "period = f\"{report_year}-{report_month:02d}\"\n",
    "\n",
    "# ── 4) Load previously processed metadata\n",
    "s3 = boto3.client(\"s3\")\n",
    "processed = set()\n",
    "tmp_meta = \"/tmp/metadata.txt\"\n",
    "try:\n",
    "    s3.download_file(ALERTS_BUCKET, METADATA_KEY, tmp_meta)\n",
    "    with open(tmp_meta, \"r\") as f:\n",
    "        processed = set(line.strip() for line in f)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] != 'NoSuchKey':\n",
    "        raise\n",
    "\n",
    "# ── 5) Load and clean data\n",
    "leave_df = (\n",
    "    spark.read.parquet(LEAVE_DATA_PATH)\n",
    "         .withColumn(\"date\", to_date(\"date\", \"yyyy-M-d\"))\n",
    "         .filter(col(\"status\") == \"ACTIVE\")\n",
    "         .dropDuplicates([\"emp_id\", \"date\"])\n",
    ")\n",
    "\n",
    "holidays_df = (\n",
    "    spark.read.option(\"header\", True).csv(LEAVE_CALENDAR_PATH)\n",
    "         .withColumn(\"date\", to_date(\"date\", \"yyyy-M-d\"))\n",
    "         .select(\"date\").distinct()\n",
    ")\n",
    "\n",
    "clean_leaves = (\n",
    "    leave_df.withColumn(\"dow\", date_format(\"date\", \"E\"))\n",
    "            .filter(~col(\"dow\").isin(\"Sat\", \"Sun\"))\n",
    "            .drop(\"dow\")\n",
    "            .join(holidays_df, on=\"date\", how=\"left_anti\")\n",
    ")\n",
    "\n",
    "quota_df = spark.read.option(\"header\", True).csv(LEAVE_QUOTA_PATH)\n",
    "\n",
    "# ── 6) Filter only leaves in the current reporting year up to the reporting month\n",
    "reporting_year = report_year\n",
    "\n",
    "up_to = clean_leaves.filter(\n",
    "    (year(\"date\") == reporting_year) &\n",
    "    (month(\"date\") <= report_month)\n",
    ")\n",
    "\n",
    "# ── 7) Count leave days per employee\n",
    "counts_df = up_to.groupBy(\"emp_id\").agg(countDistinct(\"date\").alias(\"used\"))\n",
    "\n",
    "# ── 8) Compare with quota and find violators\n",
    "breachers_df = (\n",
    "    counts_df.join(quota_df, on=\"emp_id\", how=\"inner\")\n",
    "             .filter((col(\"used\") / col(\"leave_quota\")) > 0.8)\n",
    "             .select(\"emp_id\")\n",
    ")\n",
    "\n",
    "# ── 9) Avoid duplicates\n",
    "to_alert = [\n",
    "    emp for emp in breachers_df.collect()\n",
    "    if f\"{period},{emp.emp_id}\" not in processed\n",
    "]\n",
    "\n",
    "if not to_alert:\n",
    "    print(f\"No new alerts for {period}\")\n",
    "\n",
    "# ── 10) Write alerts to S3 text file\n",
    "ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "out_path = f\"{OUTPUT_PATH}{period}/run-{ts}/\"\n",
    "\n",
    "lines_df = spark.createDataFrame(\n",
    "    [(e.emp_id, period) for e in to_alert],\n",
    "    [\"emp_id\", \"month\"]\n",
    ").select(concat_ws(\",\", \"emp_id\", \"month\").alias(\"line\"))\n",
    "\n",
    "lines_df.coalesce(1).write.mode(\"overwrite\").text(out_path)\n",
    "print(f\"✅ Alerted {len(to_alert)} employees → {out_path}\")\n",
    "\n",
    "# ── 11) Update metadata\n",
    "with open(tmp_meta, \"a\") as f:\n",
    "    for emp in to_alert:\n",
    "        f.write(f\"{period},{emp.emp_id}\\n\")\n",
    "s3.upload_file(tmp_meta, ALERTS_BUCKET, METADATA_KEY)\n",
    "os.remove(tmp_meta)\n",
    "\n",
    "# ── 12) Finish\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b6dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955c2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a51f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726740e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
