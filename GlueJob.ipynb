{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18277277",
   "metadata": {},
   "source": [
    "Task 1 - Employee_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e12b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_data/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_data_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# Expected columns\n",
    "expected_columns = {\"emp_id\", \"age\", \"name\"}\n",
    "\n",
    "# Initialize Spark and boto3\n",
    "spark = SparkSession.builder.appName(\"DailyEmployeeProcessing\").getOrCreate()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Ensure processed files log exists\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = {line.split(',')[0]: float(line.split(',')[1]) for line in obj['Body'].read().decode('utf-8').splitlines() if line}\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        processed_files = {}\n",
    "\n",
    "    # List all CSV files in bronze\n",
    "    files = s3.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix).get('Contents', [])\n",
    "    csv_files = [f for f in files if f['Key'].endswith('.csv')]\n",
    "\n",
    "    # Identify new/updated files\n",
    "    to_process = []\n",
    "    for file in csv_files:\n",
    "        key = file['Key']\n",
    "        last_modified = file['LastModified'].timestamp()\n",
    "        if processed_files.get(key) != last_modified:\n",
    "            to_process.append((key, last_modified))\n",
    "\n",
    "    if not to_process:\n",
    "        print(\"INFO: No new or updated files to process. Exiting gracefully.\")\n",
    "    else:\n",
    "        for key, last_modified in to_process:\n",
    "            print(f\"Processing file: {key}\")\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"s3://{bucket_name}/{key}\")\n",
    "\n",
    "            # Check if all expected columns are present\n",
    "            actual_columns = set(df.columns)\n",
    "            missing = expected_columns - actual_columns\n",
    "            if missing:\n",
    "                raise ValueError(f\"ERROR: Missing expected columns in {key}: {missing}\")\n",
    "\n",
    "            # Clean and validate\n",
    "            cleaned_df = df.select(\n",
    "                col(\"emp_id\").cast(\"string\"),\n",
    "                col(\"age\").cast(\"int\"),\n",
    "                col(\"name\").cast(\"string\")\n",
    "            ).dropna().dropDuplicates()\n",
    "\n",
    "            # Write cleaned data to gold path\n",
    "            cleaned_df.write.mode(\"append\").parquet(gold_path)\n",
    "\n",
    "            # Update log\n",
    "            processed_files[key] = last_modified\n",
    "\n",
    "        # Write updated log back to S3\n",
    "        log_content = \"\\n\".join([f\"{k},{v}\" for k, v in processed_files.items()])\n",
    "        s3.put_object(Bucket=bucket_name, Key=processed_file_key, Body=log_content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Job failed with exception: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5335378",
   "metadata": {},
   "source": [
    "Task 2 - leave_quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88294028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import boto3\n",
    "\n",
    "# Define S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_leave_quota/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_leave_quota_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# Initialize Spark and Boto3\n",
    "spark = SparkSession.builder.appName(\"YearlyLeaveQuotaTableJob\").getOrCreate()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Step 1: Load processed files\n",
    "    try:\n",
    "        processed_obj = s3_client.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = processed_obj['Body'].read().decode('utf-8').splitlines()\n",
    "    except s3_client.exceptions.ClientError:\n",
    "        processed_files = []\n",
    "\n",
    "    processed_files_set = set(processed_files)  # Faster lookup\n",
    "\n",
    "    # Step 2: List all files under bronze_prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix)\n",
    "    all_files = response.get('Contents', [])\n",
    "\n",
    "    # Filter only CSV files excluding processed_files.txt itself\n",
    "    new_files = [obj['Key'] for obj in all_files if obj['Key'].endswith('.csv') and obj['Key'] not in processed_files_set]\n",
    "\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "    else:\n",
    "        for file_key in new_files:\n",
    "            file_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "            file_name = file_key.split(\"/\")[-1]  # Get only filename\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Step 3: Read the file\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_cols = [\"emp_id\", \"leave_quota\", \"year\"]\n",
    "            if not all(c in df.columns for c in required_cols):\n",
    "                print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Clean and transform\n",
    "            cleaned_df = (\n",
    "                df.select(\n",
    "                    col(\"emp_id\").cast(StringType()),\n",
    "                    col(\"leave_quota\").cast(IntegerType()),\n",
    "                    col(\"year\").cast(IntegerType())\n",
    "                )\n",
    "                .filter(\n",
    "                    col(\"emp_id\").isNotNull() &\n",
    "                    col(\"leave_quota\").isNotNull() &\n",
    "                    col(\"year\").isNotNull()\n",
    "                )\n",
    "                .dropDuplicates([\"emp_id\", \"year\"])\n",
    "            )\n",
    "\n",
    "            # Step 5: Write to Gold\n",
    "            print(f\"Writing {file_name} data to {gold_path}\")\n",
    "            cleaned_df.write.mode(\"append\").partitionBy(\"year\").parquet(gold_path)\n",
    "\n",
    "            # Step 6: After successful write, mark as processed\n",
    "            processed_files_set.add(file_key)\n",
    "\n",
    "        # Step 7: Update processed_files.txt\n",
    "        updated_content = \"\\n\".join(processed_files_set)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=processed_file_key, Body=updated_content)\n",
    "        print(\"Processed files list updated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4becf8",
   "metadata": {},
   "source": [
    "Task 3 - leave_calender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eedb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "import boto3\n",
    "\n",
    "# S3 paths\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_prefix = \"poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "gold_path = f\"s3://{bucket_name}/poc-bootcamp-gold/employee_leave_calendar_output/\"\n",
    "processed_file_key = f\"{bronze_prefix}processed_files.txt\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YearlyLeaveCalendarTableJob\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    # Step 1: Load processed files\n",
    "    try:\n",
    "        processed_obj = s3_client.get_object(Bucket=bucket_name, Key=processed_file_key)\n",
    "        processed_files = processed_obj['Body'].read().decode('utf-8').splitlines()\n",
    "    except s3_client.exceptions.ClientError:\n",
    "        processed_files = []\n",
    "\n",
    "    processed_files_set = set(processed_files)\n",
    "\n",
    "    # Step 2: List all files under bronze_prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bronze_prefix)\n",
    "    all_files = response.get('Contents', [])\n",
    "\n",
    "    # Filter only CSV files excluding processed_files.txt itself\n",
    "    new_files = [obj['Key'] for obj in all_files if obj['Key'].endswith('.csv') and obj['Key'] not in processed_files_set]\n",
    "\n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "    else:\n",
    "        for file_key in new_files:\n",
    "            file_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "            file_name = file_key.split(\"/\")[-1]  # Get only filename\n",
    "\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Step 3: Read the file\n",
    "            df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_path)\n",
    "\n",
    "            # Validate required columns\n",
    "            required_cols = [\"date\", \"reason\"]\n",
    "            if not all(c in df.columns for c in required_cols):\n",
    "                print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Clean and transform\n",
    "            cleaned_df = (\n",
    "                df.select(\n",
    "                    col(\"date\").cast(DateType()),\n",
    "                    col(\"reason\").cast(StringType())\n",
    "                )\n",
    "                .filter(\n",
    "                    col(\"date\").isNotNull() &\n",
    "                    col(\"reason\").isNotNull()\n",
    "                )\n",
    "                .dropDuplicates([\"date\", \"reason\"]) \n",
    "            )\n",
    "\n",
    "            # Add year column\n",
    "            cleaned_df = cleaned_df.withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "            # Step 5: Write to Gold\n",
    "            print(f\"Writing {file_name} data to {gold_path}\")\n",
    "            cleaned_df.write.mode(\"append\").partitionBy(\"year\").parquet(gold_path)\n",
    "\n",
    "            # Step 6: After successful write, mark as processed\n",
    "            processed_files_set.add(file_key)\n",
    "\n",
    "        # Step 7: Update processed_files.txt\n",
    "        updated_content = \"\\n\".join(processed_files_set)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=processed_file_key, Body=updated_content)\n",
    "        print(\"Processed files list updated.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Job failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a21cd2",
   "metadata": {},
   "source": [
    "Task 4 - leave_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30982b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_date, to_date,\n",
    "    sum as sum_, when, lit, year, month, broadcast\n",
    ")\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Constants\n",
    "BRONZE_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_data/\"\n",
    "CONSOLIDATED = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "\n",
    "# SparkSession (assumed auto-created in Glue)\n",
    "spark = SparkSession.builder.appName(\"DailyLeaveSnapshot\").getOrCreate()\n",
    "\n",
    "# 1) LOAD raw events and cast date\n",
    "raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .csv(BRONZE_PATH)\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-M-d\"))\n",
    ")\n",
    "\n",
    "# 2) AGGREGATE to find dominant status per emp/date\n",
    "raw_agg = (\n",
    "    raw.groupBy(\"emp_id\", \"date\")\n",
    "    .agg(\n",
    "        sum_(when(col(\"status\") == \"ACTIVE\", 1).otherwise(0)).alias(\"cnt_active\"),\n",
    "        sum_(when(col(\"status\") == \"CANCELLED\", 1).otherwise(0)).alias(\"cnt_cancelled\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"new_status\",\n",
    "        when(col(\"cnt_active\") > col(\"cnt_cancelled\"), lit(\"ACTIVE\"))\n",
    "        .otherwise(lit(\"CANCELLED\"))  # includes tie and CANCELLED majority cases\n",
    "    )\n",
    "    .select(\"emp_id\", \"date\", \"new_status\")\n",
    ")\n",
    "\n",
    "# 3) READ previous snapshot (if any)\n",
    "try:\n",
    "    hist = (\n",
    "        spark.read\n",
    "        .parquet(CONSOLIDATED)\n",
    "        .select(\"emp_id\", \"date\", \"status\", \"ingestion_date\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    # If no history exists, create an empty DataFrame with the required schema.\n",
    "    hist = spark.createDataFrame(\n",
    "        [],\n",
    "        schema=raw_agg.schema.add(\"status\", StringType()).add(\"ingestion_date\", DateType())\n",
    "    )\n",
    "\n",
    "# 4) MERGE logic: outer-join of aggregated (raw_agg) & history\n",
    "merged = hist.alias(\"h\").join(\n",
    "    raw_agg.alias(\"r\"),\n",
    "    on=[\"emp_id\", \"date\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# 5) Decide final status and ingestion_date\n",
    "today = current_date()\n",
    "result = (\n",
    "    merged.withColumn(\n",
    "        \"final_status\",\n",
    "        when(col(\"r.new_status\").isNotNull(), col(\"r.new_status\"))\n",
    "        .otherwise(col(\"h.status\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ingestion_date\",\n",
    "        when(col(\"r.new_status\").isNotNull(), today)\n",
    "        .otherwise(col(\"h.ingestion_date\"))\n",
    "    )\n",
    "    .filter(col(\"final_status\").isNotNull())\n",
    "    .select(\n",
    "        col(\"emp_id\"),\n",
    "        col(\"date\"),\n",
    "        col(\"final_status\").alias(\"status\"),\n",
    "        col(\"ingestion_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 6) Add partitions based on the leave date\n",
    "result_with_partition = (\n",
    "    result\n",
    "    .withColumn(\"year\", year(col(\"date\")))\n",
    "    .withColumn(\"month\", month(col(\"date\")))\n",
    ")\n",
    "\n",
    "####################################\n",
    "# NEW FUNCTIONALITY: Broadcast join with Employee Timeframe data\n",
    "####################################\n",
    "\n",
    "# Define the path to the employee timeframe data in the Gold layer\n",
    "employee_timeframe_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "\n",
    "# Load the employee timeframe data and filter to get only active employees.\n",
    "# (Assuming the timeframe data includes a 'status' column.)\n",
    "employee_active = (\n",
    "    spark.read.parquet(employee_timeframe_path)\n",
    "    .filter(col(\"status\") == \"ACTIVE\")\n",
    "    .select(\"emp_id\")\n",
    "    .distinct()\n",
    "    # Rename for clarity during join.\n",
    "    .withColumnRenamed(\"emp_id\", \"active_emp_id\")\n",
    ")\n",
    "\n",
    "# Perform a broadcast join to check if each employee in the leave data is active.\n",
    "# Then, update the leave status:\n",
    "# - If the employee is NOT active (no match found), force the status to \"CANCELLED\".\n",
    "# - Otherwise, retain the existing leave status from the aggregation logic.\n",
    "result_with_partition = result_with_partition.join(\n",
    "    broadcast(employee_active),\n",
    "    result_with_partition.emp_id == col(\"active_emp_id\"),\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"status\",\n",
    "    when(col(\"active_emp_id\").isNull(), lit(\"CANCELLED\")).otherwise(col(\"status\"))\n",
    ").drop(\"active_emp_id\")\n",
    "\n",
    "####################################\n",
    "# 7) Write the final result to Gold with partitions\n",
    "####################################\n",
    "(\n",
    "    result_with_partition\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .parquet(CONSOLIDATED)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1462e",
   "metadata": {},
   "source": [
    "Task 5 - timeframe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c33b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final glue job\n",
    "\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import boto3\n",
    "\n",
    "# Setup GlueContext and Spark session\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark and S3 client\n",
    "spark = SparkSession.builder.appName(\"BronzeToSilverToGold\").getOrCreate()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Paths (S3 locations)\n",
    "bucket_name = \"poc-bootcamp-capstone-group3\"\n",
    "bronze_folder = \"poc-bootcamp-bronze/employee_timeframe_data/\"\n",
    "gold_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "log_file_key = \"poc-bootcamp-bronze/employee_timeframe_data/processed_files.txt\"  # S3 log file for processed files\n",
    "\n",
    "# Today's date\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Ensure processed files log exists in S3\n",
    "try:\n",
    "    s3_client.head_object(Bucket=bucket_name, Key=log_file_key)\n",
    "except s3_client.exceptions.ClientError:\n",
    "    print(\"🚫 Log file not found. Creating a new log file in S3.\")\n",
    "    # Create an empty log file if it doesn't exist\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=log_file_key, Body=\"\")\n",
    "\n",
    "# Read the list of processed files from the log (stored in S3)\n",
    "processed_files_obj = s3_client.get_object(Bucket=bucket_name, Key=log_file_key)\n",
    "processed_files_lines = processed_files_obj['Body'].read().decode().splitlines()\n",
    "\n",
    "# Parse processed files log into a dictionary with last modified timestamps\n",
    "processed_files = {}\n",
    "for line in processed_files_lines:\n",
    "    file_name, last_modified = line.split(',')\n",
    "    processed_files[file_name] = float(last_modified)\n",
    "\n",
    "# List new files to be processed from the Bronze folder in S3\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=bronze_folder)\n",
    "\n",
    "# Check if there are files in the Bronze folder\n",
    "if 'Contents' in response:\n",
    "    all_files = [content['Key'] for content in response['Contents'] if content['Key'].endswith(\".csv\")]\n",
    "else:\n",
    "    all_files = []\n",
    "\n",
    "print(f\"Files found in Bronze folder: {all_files}\")\n",
    "\n",
    "# Identify new or modified files by checking last modified time\n",
    "new_files = []\n",
    "for file in all_files:\n",
    "    # Get last modified time of the file in S3\n",
    "    file_last_modified_time = s3_client.head_object(Bucket=bucket_name, Key=file)['LastModified'].timestamp()\n",
    "\n",
    "    # Check if the file is new or has been modified (not in processed files or modified time is different)\n",
    "    if file not in processed_files or processed_files[file] != file_last_modified_time:\n",
    "        new_files.append(file)\n",
    "\n",
    "if not new_files:\n",
    "    print(\"🚫 No new files to process.\")\n",
    "else:\n",
    "    print(f\"✅ Found {len(new_files)} new files to process.\")\n",
    "\n",
    "    # Load new data from S3 into a DataFrame\n",
    "    new_data_paths = [f\"s3://{bucket_name}/{file}\" for file in new_files]\n",
    "    new_df = spark.read.csv(new_data_paths, header=True, inferSchema=True)\n",
    "\n",
    "    # Basic processing for Silver\n",
    "    silver_df = new_df.withColumn(\"start_date\", to_date(from_unixtime(col(\"start_date\")))) \\\n",
    "                      .withColumn(\"end_date\", to_date(from_unixtime(col(\"end_date\"))))\n",
    "\n",
    "    # Write new Silver data for today (append mode)\n",
    "    silver_today_path = f\"s3://{bucket_name}/poc-bootcamp-silver/employee_timeframe_data/{today_date}/\"\n",
    "    silver_df.write.mode(\"append\").parquet(silver_today_path)\n",
    "    print(f\"✅ Written processed data to Silver layer for date {today_date}.\")\n",
    "\n",
    "    # Update processed files log in S3 with last modified time\n",
    "    new_log_content = \"\"\n",
    "    for file in new_files:\n",
    "        file_last_modified_time = s3_client.head_object(Bucket=bucket_name, Key=file)['LastModified'].timestamp()\n",
    "        new_log_content += f\"{file},{file_last_modified_time}\\n\"\n",
    "\n",
    "    # Write the updated log back to S3\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=log_file_key, Body=new_log_content)\n",
    "    print(\"📝 Processed files log updated.\")\n",
    "\n",
    "    # =====================\n",
    "    # Silver ➔ Gold\n",
    "    # =====================\n",
    "    # Directly use the silver_df already in memory (NO re-reading from disk!)\n",
    "    # Saves time and avoids overhead\n",
    "\n",
    "    try:\n",
    "        existing_gold_df = spark.read.parquet(gold_path)\n",
    "        print(\"✅ Existing Gold data found.\")\n",
    "    except Exception as e:\n",
    "        print(\"🚫 No existing Gold data found. Creating new.\")\n",
    "        existing_gold_df = spark.createDataFrame([], silver_df.schema.add(\"status\", \"string\"))\n",
    "\n",
    "    # Union new Silver data and old Gold data\n",
    "    combined_df = existing_gold_df.unionByName(silver_df.withColumn(\"status\", lit(None)), allowMissingColumns=True)\n",
    "\n",
    "    # Deduplication based on highest salary\n",
    "    window_spec = Window.partitionBy(\"emp_id\", \"start_date\", \"end_date\").orderBy(col(\"salary\").desc())\n",
    "    deduplicated_df = combined_df.withColumn(\"row_num\", row_number().over(window_spec)).filter(\"row_num = 1\").drop(\"row_num\")\n",
    "\n",
    "    # Fill missing end_date by looking ahead\n",
    "    window_emp = Window.partitionBy(\"emp_id\").orderBy(\"start_date\")\n",
    "    deduplicated_df = deduplicated_df.withColumn(\"next_start_date\", lead(\"start_date\").over(window_emp))\n",
    "    deduplicated_df = deduplicated_df.withColumn(\n",
    "        \"end_date\",\n",
    "        when(\n",
    "            col(\"next_start_date\").isNotNull(),\n",
    "            # expr(\"date_sub(next_start_date, 1)\")\n",
    "            col(\"next_start_date\")\n",
    "        ).otherwise(col(\"end_date\"))\n",
    "    )\n",
    "\n",
    "    # Add status (ACTIVE/INACTIVE)\n",
    "    final_df = deduplicated_df.withColumn(\n",
    "        \"status\",\n",
    "        when(col(\"end_date\").isNull(), \"ACTIVE\").otherwise(\"INACTIVE\")\n",
    "    ).select(\"emp_id\", \"start_date\", \"end_date\", \"designation\", \"salary\", \"status\")\n",
    "\n",
    "    # Write final Gold layer\n",
    "    final_df.write.mode(\"overwrite\").partitionBy(\"status\").parquet(gold_path)\n",
    "    print(\"🏆 Gold layer updated successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Commit the Glue job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2281a",
   "metadata": {},
   "source": [
    "Task 6 - Count by designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e79941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, current_date\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "# Initialize Spark and Glue contexts\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Define S3 paths\n",
    "gold_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_timeframe_data_output/\"\n",
    "output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/daily_active_employees_by_designation_output/\"\n",
    "\n",
    "# Read Gold Layer timeframe data\n",
    "df = spark.read.parquet(gold_path)\n",
    "\n",
    "# Filter records where today is between start_date and (end_date is null)\n",
    "today = datetime.utcnow().date()\n",
    "active_employees_df = df.filter(\n",
    "    (col(\"start_date\") <= today) &\n",
    "    (col(\"end_date\").isNull())\n",
    ")\n",
    "\n",
    "# Group by designation and count\n",
    "summary_df = active_employees_df.groupBy(\"designation\").count().withColumnRenamed(\"count\", \"active_count\")\n",
    "\n",
    "# Add snapshot date column for partitioning\n",
    "summary_df = summary_df.withColumn(\"snapshot_date\", current_date())\n",
    "\n",
    "# Write to S3 partitioned by snapshot_date\n",
    "summary_df.write.mode(\"append\").partitionBy(\"snapshot_date\").parquet(output_path)\n",
    "\n",
    "print(\"✅ Daily active employee snapshot generated and saved to S3.\")\n",
    "\n",
    "# Commit job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48653999",
   "metadata": {},
   "source": [
    "Task 7 - 8% Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, dayofweek, lit\n",
    "from pyspark.sql.types import DateType\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# Initialize Spark & Glue Context\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# Get job parameters\n",
    "args = getResolvedOptions(sys.argv, ['YEAR', 'TODAY_DATE'])\n",
    "\n",
    "# PARAMETERS\n",
    "YEAR = int(args['YEAR'])  # Convert YEAR to integer\n",
    "THRESHOLD_PCT =0.08\n",
    "today = datetime.datetime.strptime(args['TODAY_DATE'], '%Y-%m-%d').date()  # Convert string to date\n",
    "\n",
    "# FILE PATHS\n",
    "leaves_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "holidays_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "\n",
    "# LOAD LEAVE DATA (Parquet format)\n",
    "leaves_df = spark.read.parquet(leaves_path).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# LOAD HOLIDAY DATA (CSV format)\n",
    "holidays_df = spark.read.option(\"header\", True).csv(holidays_path).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# BUILD CALENDAR (Excluding weekends and holidays)\n",
    "start = today + datetime.timedelta(days=1)  # Start from tomorrow\n",
    "end = datetime.date(YEAR, 12, 31)  # End on December 31\n",
    "\n",
    "# Generate range of dates from tomorrow to Dec 31\n",
    "days_df = spark.range(0, (end - start).days + 1).select((lit(start) + col(\"id\").cast(\"int\")).alias(\"date\"))\n",
    "\n",
    "# Filter out weekends and holidays\n",
    "working_days_df = days_df.join(holidays_df, on=\"date\", how=\"left_anti\").filter(dayofweek(col(\"date\")).between(2, 6))\n",
    "\n",
    "total_working_days = working_days_df.count()\n",
    "print(\"Total upcoming working days:\", total_working_days)\n",
    "\n",
    "# FILTER ACTIVE LEAVES ON FUTURE WORKING DAYS (Excluding cancelled)\n",
    "active_leaves_df = (\n",
    "    leaves_df.filter((col(\"status\") == \"ACTIVE\") & (col(\"date\") > lit(today)))\n",
    "    .join(working_days_df, on=\"date\", how=\"inner\")\n",
    "    .dropDuplicates([\"emp_id\", \"date\"])\n",
    ")\n",
    "\n",
    "# COUNT LEAVE APPLICATIONS PER EMPLOYEE\n",
    "emp_leave_counts_df = active_leaves_df.groupBy(\"emp_id\").agg(countDistinct(\"date\").alias(\"upcoming_leaves\"))\n",
    "\n",
    "# APPLY 8% THRESHOLD\n",
    "result_df = emp_leave_counts_df.filter(col(\"upcoming_leaves\") > THRESHOLD_PCT * lit(total_working_days))\n",
    "\n",
    "# Write output to S3 in Parquet format\n",
    "output_path = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/8%-Threshold_output/\"\n",
    "result_df.write.parquet(output_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d9da2",
   "metadata": {},
   "source": [
    "Task 8 - 80% Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    to_date, col, year, month, countDistinct, concat_ws, lit, date_format\n",
    ")\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# ── 1) Glue setup\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# ── 2) S3 Config\n",
    "LEAVE_DATA_PATH     = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_leave_data_output/\"\n",
    "LEAVE_CALENDAR_PATH = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_calendar_data/\"\n",
    "LEAVE_QUOTA_PATH    = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-bronze/employee_leave_quota/\"\n",
    "\n",
    "ALERT_TEXT_OUTPUT_PATH   = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_80%_text_output/\"\n",
    "PARQUET_OUTPUT_PATH      = \"s3://poc-bootcamp-capstone-group3/poc-bootcamp-gold/employee_80%_parquet_output/\"\n",
    "METADATA_KEY             = \"poc-bootcamp-bronze/80%Threshold/metadata.txt\"\n",
    "ALERTS_BUCKET            = \"poc-bootcamp-capstone-group3\"\n",
    "\n",
    "# ── 3) Reference date and reporting period\n",
    "ref_date = datetime.date(2024, 11, 1)  # <-- Replace with datetime.date.today() for real-time jobs\n",
    "# ref_date = datetime.date.today()\n",
    "\n",
    "report_month = ref_date.month - 1 or 12\n",
    "report_year = ref_date.year if ref_date.month > 1 else ref_date.year - 1\n",
    "period = f\"{report_year}-{report_month:02d}\"\n",
    "\n",
    "# ── 4) Load previously processed metadata\n",
    "s3 = boto3.client(\"s3\")\n",
    "processed = set()\n",
    "tmp_meta = \"/tmp/metadata.txt\"\n",
    "try:\n",
    "    s3.download_file(ALERTS_BUCKET, METADATA_KEY, tmp_meta)\n",
    "    with open(tmp_meta, \"r\") as f:\n",
    "        processed = set(line.strip() for line in f)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] != 'NoSuchKey':\n",
    "        raise\n",
    "\n",
    "# ── 5) Load and clean data\n",
    "leave_df = (\n",
    "    spark.read.parquet(LEAVE_DATA_PATH)\n",
    "         .withColumn(\"date\", to_date(\"date\", \"yyyy-M-d\"))\n",
    "         .filter(col(\"status\") == \"ACTIVE\")\n",
    "         .dropDuplicates([\"emp_id\", \"date\"])\n",
    ")\n",
    "\n",
    "holidays_df = (\n",
    "    spark.read.option(\"header\", True).csv(LEAVE_CALENDAR_PATH)\n",
    "         .withColumn(\"date\", to_date(\"date\", \"yyyy-M-d\"))\n",
    "         .select(\"date\").distinct()\n",
    ")\n",
    "\n",
    "clean_leaves = (\n",
    "    leave_df.withColumn(\"dow\", date_format(\"date\", \"E\"))\n",
    "            .filter(~col(\"dow\").isin(\"Sat\", \"Sun\"))\n",
    "            .drop(\"dow\")\n",
    "            .join(holidays_df, on=\"date\", how=\"left_anti\")\n",
    ")\n",
    "\n",
    "quota_df = spark.read.option(\"header\", True).csv(LEAVE_QUOTA_PATH)\n",
    "\n",
    "# ── 6) Filter only leaves in the current reporting year up to the reporting month\n",
    "up_to = clean_leaves.filter(\n",
    "    (year(\"date\") == report_year) &\n",
    "    (month(\"date\") <= report_month)\n",
    ")\n",
    "\n",
    "# ── 7) Count leave days per employee\n",
    "counts_df = up_to.groupBy(\"emp_id\").agg(countDistinct(\"date\").alias(\"used\"))\n",
    "\n",
    "# ── 8) Compare with quota and find violators\n",
    "breachers_df = (\n",
    "    counts_df.join(quota_df, on=\"emp_id\", how=\"inner\")\n",
    "             .filter((col(\"used\") / col(\"leave_quota\")) > 0.8)\n",
    "             .select(\"emp_id\")\n",
    ")\n",
    "\n",
    "# ── 9) Avoid duplicates\n",
    "to_alert = [\n",
    "    emp for emp in breachers_df.collect()\n",
    "    if f\"{period},{emp.emp_id}\" not in processed\n",
    "]\n",
    "\n",
    "if not to_alert:\n",
    "    print(f\"No new alerts for {period}\")\n",
    "\n",
    "# ── 10) Write alerts to S3 text file\n",
    "ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "text_out_path = f\"{ALERT_TEXT_OUTPUT_PATH}{period}/run-{ts}/\"\n",
    "\n",
    "lines_df = spark.createDataFrame(\n",
    "    [(e.emp_id, period) for e in to_alert],\n",
    "    [\"emp_id\", \"month\"]\n",
    ").select(concat_ws(\",\", \"emp_id\", \"month\").alias(\"line\"))\n",
    "\n",
    "lines_df.coalesce(1).write.mode(\"overwrite\").text(text_out_path)\n",
    "print(f\"✅ Alerted {len(to_alert)} employees → {text_out_path}\")\n",
    "\n",
    "# ── 10.1) Write Parquet of employees who breached the limit with their used leave count\n",
    "breachers_with_count_df = (\n",
    "    breachers_df.join(counts_df, on=\"emp_id\", how=\"inner\")\n",
    "                .select(col(\"emp_id\"), col(\"used\").alias(\"count_of_leaves\"))\n",
    ")\n",
    "\n",
    "parquet_out_path = f\"{PARQUET_OUTPUT_PATH}{period}/run-{ts}/\"\n",
    "breachers_with_count_df.write.mode(\"overwrite\").parquet(parquet_out_path)\n",
    "print(f\"✅ Parquet file written with leave counts → {parquet_out_path}\")\n",
    "\n",
    "# ── 11) Update metadata\n",
    "with open(tmp_meta, \"a\") as f:\n",
    "    for emp in to_alert:\n",
    "        f.write(f\"{period},{emp.emp_id}\\n\")\n",
    "s3.upload_file(tmp_meta, ALERTS_BUCKET, METADATA_KEY)\n",
    "os.remove(tmp_meta)\n",
    "\n",
    "# ── 12) Finish\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b6dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955c2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a51f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726740e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
